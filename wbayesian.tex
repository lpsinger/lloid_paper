\documentclass{article}
\usepackage{amssymb}
\begin{document}
\title{Bayesian search for sine-Gaussian bursts}
\author{Antony C. Searle}
\maketitle
\section{What are we computing?}

Suppose we have a white noise detector that makes a vector of observations $\mathbf{x}$.  These are distributed according to
\begin{eqnarray}
p_\mathbf{0}(\mathbf{x})&\propto&\exp(-\mathbf{x}^2/2)
\end{eqnarray}
If there is some additive signal $\mathbf{y}$ as well as the noise, then $\mathbf{x}-\mathbf{y}$ will have the same distribution.
\begin{eqnarray}
p_\mathbf{y}(\mathbf{x})&\propto&\exp(-(\mathbf{x-y})^2/2)
\end{eqnarray}
The way to decide between these situations is to look at the ratio
\begin{eqnarray}
\frac{p_\mathbf{y}(\mathbf{x})}{p_\mathbf{0}(\mathbf{x})}=\frac{\exp(-(\mathbf{x-y})^2/2)}{\exp(-\mathbf{x}^2/2)}=\exp(\mathbf{xy}-\mathbf{y}^2/2)
\end{eqnarray}
Bayesian and non-Bayesian statistics agree thus far.
How do we deal with an unknown $\mathbf{y}$?  The Bayesian method is to average this ratio over $\mathbf{y}$.  We might do this by computing
\begin{eqnarray}
\frac{ \frac{1}{V}\int_V p_\mathbf{y}(\mathbf{x}) d\mathbf{y}}{p_\mathbf{0}(\mathbf{x})}
\end{eqnarray}
if $\mathbf{y}$ is constrained to lie in some region of parameter space with volume $V$.  More generally, we can define some function $q(\mathbf{y})$ with
\begin{eqnarray}
\int_{\mathbb{R}^n} q(\mathbf{y}) d\mathbf{y} = 1
\end{eqnarray}
and computing the $q$-weighted average
\begin{eqnarray}
\frac{ \int_{\mathbb{R}^n} q(\mathbf{y}) p_\mathbf{y}(\mathbf{x}) d\mathbf{y}}{p_\mathbf{0}(\mathbf{x})}
\end{eqnarray}
The search computes this expression, slightly generalized to colored noise with covariance $\mathbf{\Sigma}$ by replacing $\mathbf{x}^2$ with $\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}$ (and likewise replacing $(\mathbf{x-y})^2$ with $(\mathbf{x-y})^T\mathbf{\Sigma}^{-1}(\mathbf{x-y})$

By judicious definition of $q$, we can restrict the analysis to certain populations of signal (such as $\mathbf{y}$ always a sine-gaussian) and encode our expectations about the relative abundance of signals (such as small signals being more common than large signals).  $q$ is the Bayesian prior plausibility distribution for the signal population, and the quantity the algorithm computes is the Bayes factor.  $q$ is a subjective choice, but as any $q$ makes definite statements about the anticipated signal population, it is constrained by the need to make reasonable statements.

If we choose a $q$ that is Gaussian (or Gaussian on some of its parameters), we can solve the averaging integral analytically (or solve it for some of its parameters).

%How does the non-Bayesian statistic deal with unknown $\mathbf{y}$?  One method is to maximize the statistic for individual $\mathbf{y}$
%\begin{eqnarray}
%\max_\mathbf{y}\frac{p_\mathbf{y}(\mathbf{x})}{p_\mathbf{0}(\mathbf{x})}
%\end{eqnarray}
%or maximize with $\mathbf{y}$ constrained to some subset of parameter space $S$
%\begin{eqnarray}
%\max_{\mathbf{y}\in S}\frac{p_\mathbf{y}(\mathbf{x})}{p_\mathbf{0}(\mathbf{x})}
%\end{eqnarray}
%or maximize the expression biased by some regulating function $\alpha(\mathbf{y})$
%\begin{eqnarray}
%\max_\mathbf{y}\frac{p_\mathbf{y}(\mathbf{x})}{p_\mathbf{0}(\mathbf{x})}\alpha(\mathbf{y})
%\end{eqnarray}

\section{Notation}

For an explanation of notation, refer to the draft Bayesian paper by browsing the lscsoft repository

\begin{verbatim}
http://www.lsc-group.phys.uwm.edu/cgi-bin/cvs/viewcvs.cgi/?cvsroot=lscsoft
\end{verbatim}

for

\begin{verbatim}
/matapps/src/searches/burst/coherent-network/doc/bayesian/
\end{verbatim}

or at the link below

\begin{verbatim}
http://www.lsc-group.phys.uwm.edu/cgi-bin/cvs/viewcvs.cgi/
*checkout*/matapps/src/searches/burst/coherent-network/doc/
bayesian/bayesian.pdf?rev=1.42&cvsroot=lscsoft&content-type
=application/pdf
\end{verbatim}

\section{Analysis}
The Bayes factor for a burst search with multivariate normal distribution priors on noise and signal waveform is
\begin{eqnarray}
\frac{
p(\mathbf{x}|H_\textrm{signal},I)
}{
p(\mathbf{x}|H_\textrm{noise},I)
}
&=&
\int_{-\infty}^{+\infty}\ldots\int_{-\infty}^{+\infty}
p(\mathbf{\rho},\tau,\theta,\phi|H_\textrm{signal},I)
\sqrt{\det(\mathbf{I}-\mathbf{\Sigma}\mathbf{K})}
\exp(\frac{1}{2}\mathbf{x}^T\mathbf{K}\mathbf{x})
\mathrm{d}\mathbf{\rho}\mathrm{d}\tau\mathrm{d}\theta\mathrm{d}\phi\nonumber
\end{eqnarray}
where the kernel is
\begin{eqnarray}
\mathbf{K}(\mathbf{\rho},\tau,\theta,\phi)&=&
(\mathbf{\Sigma}^{-1}\mathbf{F}\mathbf{W})
(
(\mathbf{F}\mathbf{W})^T
\mathbf{\Sigma}^{-1}
\mathbf{F}\mathbf{W}
+
\mathbf{A}^{-1}
)^{-1}
(\mathbf{\Sigma}^{-1}\mathbf{F}\mathbf{W})^T.\nonumber
\end{eqnarray}
The matrix $\mathbf{W}$ contains the sine- and cosine-Gaussian waveforms for each polarization.  The matrix $\mathbf{A}$ specifies the distribution of the amplitudes of those waveforms.  The matrix $\mathbf{F}$ is the detector response, mapping the waveforms onto the output by considering the effects of directional sensitivity and time-delay.

In this case, our signal model's extra variables bundled up in $\rho$ are central frequency $f$, quality factor $Q$ and amplitude $\sigma$.

For a system of detectors with independent noise,
\begin{eqnarray}
\mathbf{\Sigma}&=&\left[
\begin{array}{cccc}
\mathbf{\Sigma}_1 & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{0} & \mathbf{\Sigma}_2 & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{\Sigma}_N \\
\end{array}
\right].
\end{eqnarray}
For data that is appropriately timeshifted before analysis,
\begin{eqnarray}
\mathbf{F}&=&\left[
\begin{array}{cc}
F_1^+\mathbf{I} & F_1^\times\mathbf{I} \\
F_2^+\mathbf{I} & F_2^\times\mathbf{I} \\
\vdots & \vdots \\
F_N^+\mathbf{I} & F_N^\times\mathbf{I} \\
\end{array}
\right].
\end{eqnarray}
For sine-Gaussians, the waveforms are a vector $\mathbf{w}_1$ sampling the cosine-Gaussian and a vector $\mathbf{w}_2$ sampling the sine-Gaussian, so the model is
\begin{eqnarray}
\mathbf{W}&=&\left[
\begin{array}{cccc}
\mathbf{w}_1 & \mathbf{w}_2 & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{w}_1 & \mathbf{w}_2
\end{array}
\right]
\end{eqnarray}
and the amplitude prior covariance matrix
\begin{eqnarray}
\mathbf{A}&=&\left[
\begin{array}{cccc}
\sigma^2 & 0 & 0 & 0 \\
0 & \sigma^2 & 0 & 0 \\
0 & 0 & \sigma^2 & 0 \\
0 & 0 & 0 & \sigma^2 \\
\end{array}
\right].
\end{eqnarray}
We do not rely on any properties of sine-Gaussians, so the derivation holds for any two waveforms, and generalizes to any number of waveforms (though for many waveforms it will no longer be fast.)

Therefore,
\begin{eqnarray}
\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{FW}&=&
\mathbf{x}^T\left[
\begin{array}{cccc}
\mathbf{\Sigma}_1^{-1} & \mathbf{0} & \cdots & \mathbf{0} \\
\mathbf{0} & \mathbf{\Sigma}_2^{-1} & \cdots & \mathbf{0} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{0} & \mathbf{0} & \cdots & \mathbf{\Sigma}_N^{-1} \\
\end{array}
\right]
\left[
\begin{array}{cc}
F_1^+\mathbf{I} & F_1^\times\mathbf{I} \\
F_2^+\mathbf{I} & F_2^\times\mathbf{I} \\
\vdots & \vdots \\
F_N^+\mathbf{I} & F_N^\times\mathbf{I} \\
\end{array}
\right]\mathbf{W}
\\
&=&
\mathbf{x}^T\left[
\begin{array}{cc}
F_1^+\mathbf{\Sigma}_1^{-1} & F_1^\times\mathbf{\Sigma}_1^{-1} \\
F_2^+\mathbf{\Sigma}_2^{-1} & F_2^\times\mathbf{\Sigma}_2^{-1} \\
\vdots & \vdots \\
F_3^+\mathbf{\Sigma}_3^{-1} & F_3^\times\mathbf{\Sigma}_3^{-1}
\end{array}
\right]\left[
\begin{array}{cccc}
\mathbf{w}_1 & \mathbf{w}_2 & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{w}_1 & \mathbf{w}_2
\end{array}
\right]\\
&=&
\left[
\begin{array}{c}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\vdots\\
\mathbf{x}_N
\end{array}
\right]^T
\left[
\begin{array}{cccc}
F_1^+\mathbf{\Sigma}_1^{-1}\mathbf{w}_1 & F_1^+\mathbf{\Sigma}_1^{-1}\mathbf{w}_2 & F_1^\times\mathbf{\Sigma}_1^{-1}\mathbf{w}_1 & F_1^\times\mathbf{\Sigma}_1^{-1}\mathbf{w}_2
\\
F_2^+\mathbf{\Sigma}_2^{-1}\mathbf{w}_1 & F_2^+\mathbf{\Sigma}_2^{-1}\mathbf{w}_2 & F_2^\times\mathbf{\Sigma}_2^{-1}\mathbf{w}_1 & F_2^\times\mathbf{\Sigma}_2^{-1}\mathbf{w}_2
\\
\vdots & \vdots & \vdots & \vdots
\\
F_N^+\mathbf{\Sigma}_N^{-1}\mathbf{w}_1 & F_N^+\mathbf{\Sigma}_N^{-1}\mathbf{w}_2 & F_N^\times\mathbf{\Sigma}_N^{-1}\mathbf{w}_1 & F_N^\times\mathbf{\Sigma}_N^{-1}\mathbf{w}_2
\end{array}
\right]\\
&=&
\left[
\begin{array}{c}
\sum_{i=1}^N F_i^+\mathbf{x}_i^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 \\
\sum_{i=1}^N F_i^+\mathbf{x}_i^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 \\
\sum_{i=1}^N F_i^\times\mathbf{x}_i^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 \\
\sum_{i=1}^N F_i^\times\mathbf{x}_i^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2
\end{array}
\right]^T,
\end{eqnarray}
a 4-vector.

Note that
\begin{eqnarray}
\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_i\mathbf{w}_j
\end{eqnarray}
is a scalar value, and is the (unnormalized) matched filter of the data from a detector with one of the two waveforms.  There are $2N$ such values, and they are expensive to compute (a Q-transform computes 2 at a time: the real and imaginary components of the complex amplitude).  However, the quantity does not depend on $\mathbf{F}$, so the values are not functions of direction.

The middle  of the kernel,
\begin{eqnarray}
(
(\mathbf{F}\mathbf{W})^T
\mathbf{\Sigma}^{-1}
\mathbf{F}\mathbf{W}
+
\mathbf{A}^{-1}
)^{-1}
\end{eqnarray}
is a function of direction, but it is only a 4-by-4 matrix.  It can be computed once and used for many arrival times (so long as the noise is stationary).

We can therefore cheaply compute the data-dependent portion of the Bayes factor by forming the matched filters for each detector individually, and for each direction on the sky performing the small number of operations required to combine the matched filter values.  If the detectors are not co-located, the matched filters will have to be computed at direction dependent time shifts, \emph{but} \texttt{wtransform} already produces an oversampled time series of filter outputs from which which we can cheaply look-up and/or interpolate the required values, so long as we are searching over arrival time as well as direction.

The size of the middle of the kernel is determined by the number of parameters in the signal model.  For white noise bursts, the number of basis waveforms and parameters is large and the implementation above will be slower than a more straightforward one.

The derivation above applies to any basis.  The Fourier basis is obviously a convenient one; it diagonalizes $\mathbf{\Sigma}^{-1}$ (for ``circular'' stationary noise).  The matched filters can be computed by doubly-whitening the data before applying a modified \texttt{wtransform} that returns the complex amplitude rather than the normalized energy.

The implementation performs only as many Q-transforms as the corresponding incoherent search.  The cost of marginalizing over arrival time, direction, frequency, Q (and possibly amplitude) will be significant, but should be much faster than a signal and null stream based search that has to perform as many Q transforms for \emph{every} direction considered.

We can also accelerate the computation of the middle part of the kernel
\begin{eqnarray}
\mathbf{FW}&=&
\left[
\begin{array}{cc}
F_1^+\mathbf{I} & F_1^\times\mathbf{I} \\
F_2^+\mathbf{I} & F_2^\times\mathbf{I} \\
\vdots & \vdots \\
F_N^+\mathbf{I} & F_N^\times\mathbf{I} \\
\end{array}
\right]\left[
\begin{array}{cccc}
\mathbf{w}_1 & \mathbf{w}_2 & \mathbf{0} & \mathbf{0} \\
\mathbf{0} & \mathbf{0} & \mathbf{w}_1 & \mathbf{w}_2
\end{array}
\right]\\
&=&
\left[
\begin{array}{cccc}
F_1^+\mathbf{w}_1 & F_1^+\mathbf{w}_2 & F_1^\times\mathbf{w}_1 & F_1^\times\mathbf{w}_2 \\
F_2^+\mathbf{w}_1 & F_2^+\mathbf{w}_2 & F_2^\times\mathbf{w}_1 & F_2^\times\mathbf{w}_2 \\
\vdots & \vdots & \vdots & \vdots \\
F_N^+\mathbf{w}_1 & F_N^+\mathbf{w}_2 & F_N^\times\mathbf{w}_1 & F_N^\times\mathbf{w}_2
\end{array}
\right]\\
\mathbf{(FW)}^T\mathbf{\Sigma}^{-1}\mathbf{FW}&=&
\left[
\begin{array}{cccc}
F_1^+\mathbf{w}_1^T & F_2^+\mathbf{w}^T_1 & \cdots & F_N^+\mathbf{w}^T_1 \\
F_1^+\mathbf{w}_2^T & F_2^+\mathbf{w}^T_2 & \cdots & F_N^+\mathbf{w}^T_2 \\
F_1^\times\mathbf{w}^T_1 & F_2^\times\mathbf{w}^T_1 & \cdots & F_N^\times\mathbf{w}^T_1 \\
F_1^\times\mathbf{w}^T_2 & F_2^\times\mathbf{w}^T_2 & \cdots & F_N^\times\mathbf{w}^T_2
\end{array}
\right]\\
&&\cdot
\left[
\begin{array}{cccc}
F_1^+\mathbf{\Sigma}_1^{-1}\mathbf{w}_1 & F_1^+\mathbf{\Sigma}_1^{-1}\mathbf{w}_2 & F_1^\times\mathbf{\Sigma}_1^{-1}\mathbf{w}_1 & F_1^\times\mathbf{\Sigma}_1^{-1}\mathbf{w}_2
\\
F_2^+\mathbf{\Sigma}_2^{-1}\mathbf{w}_1 & F_2^+\mathbf{\Sigma}_2^{-1}\mathbf{w}_2 & F_2^\times\mathbf{\Sigma}_2^{-1}\mathbf{w}_1 & F_2^\times\mathbf{\Sigma}_2^{-1}\mathbf{w}_2
\\
\vdots & \vdots & \vdots & \vdots
\\
F_N^+\mathbf{\Sigma}_N^{-1}\mathbf{w}_1 & F_N^+\mathbf{\Sigma}_N^{-1}\mathbf{w}_2 & F_N^\times\mathbf{\Sigma}_N^{-1}\mathbf{w}_1 & F_N^\times\mathbf{\Sigma}_N^{-1}\mathbf{w}_2
\end{array}
\right]\\
&=&
\end{eqnarray}
\begin{eqnarray}
\left[
\begin{array}{cccc}
\sum_{i=1}^N F_i^+F_i^+\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^+F_i^+\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 &
\sum_{i=1}^N F_i^+F_i^\times\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^+F_i^\times\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 \\
\sum_{i=1}^N F_i^+F_i^+\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^+F_i^+\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 &
\sum_{i=1}^N F_i^+F_i^\times\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^+F_i^\times\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 \\
\sum_{i=1}^N F_i^\times F_i^+\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^\times F_i^+\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 &
\sum_{i=1}^N F_i^\times F_i^\times\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^\times F_i^\times\mathbf{w}_1^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 \\
\sum_{i=1}^N F_i^\times F_i^+\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^\times F_i^+\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2 &
\sum_{i=1}^N F_i^\times F_i^\times\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_1 &
\sum_{i=1}^N F_i^\times F_i^\times\mathbf{w}_2^T\mathbf{\Sigma}_i^{-1}\mathbf{w}_2
\end{array}
\right]
\end{eqnarray}
so we can rapidly compute the middle of the kernel from the $4N$ direction-independent scalars \begin{eqnarray}
\mathbf{w}_i^T\mathbf{\Sigma}_j^{-1}\mathbf{w}_k
\end{eqnarray}
which seem to be the normalization terms for the matched filter.  (Symmetry reduces it to $3N$, with orthogonal waveforms reducing to $2N$)

In fact, the definition of the matched filter is
\begin{eqnarray}
\frac{\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{w}}{\sqrt{\mathbf{w}^T\mathbf{\Sigma}^{-1}\mathbf{w}}}
\end{eqnarray}
and we reduce to (the square of) it for one detector, one waveform, $\mathbf{F} = \mathbf{I}$, and $\sigma=\infty$.
\begin{eqnarray}
\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{w}(\mathbf{w}^T\mathbf{\Sigma}^{-1}\mathbf{w})^{-1}\mathbf{w}^T\mathbf{\Sigma}^{-1}\mathbf{x}
\end{eqnarray}

To compute the determinant,
\begin{eqnarray}
|\mathbf{I}-\mathbf{\Sigma K}|&=&|\mathbf{I}-\mathbf{\Sigma}\mathbf{\Sigma}^{-1}\mathbf{FW}((\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}(\mathbf{\Sigma}^{-1}\mathbf{FW})^T|\\
&=&|\mathbf{I}-\mathbf{FW}((\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}(\mathbf{\Sigma}^{-1}\mathbf{FW})^T|\\
&=&|\mathbf{I}-(\mathbf{\Sigma}^{-1}\mathbf{FW})^T\mathbf{FW}((\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}|\\
&=&|\mathbf{I}-(\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}((\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}|
\end{eqnarray}
(by Sylvester's theorem).  The matrices $(\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}$ and $((\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}$ are only 4-by-4 and already need to be computed for the kernel.

Straightforward implementation was found to run 14,000 slower than real time.  Despite this, the operation is quite efficient: it computes the Bayes factor for $10^6$ hypotheses per second.  The problem is that we explicitly consider too many hypotheses.

For small $N$ we may want to recast these even further to get the kernel, an $N$-by-$N$ matrix, noting that we have already found
\begin{eqnarray}
\mathbf{\Sigma}^{-1}\mathbf{FW}
\end{eqnarray}
Since the expression involves an inverse, there's no point expanding it here or in the code, except to note that explicitly computing $\mathbf{K}$ is possible and desirable in some cases.

\section{Integration}

The straightforward implementation of the algorithm above is still prohibitively expensive because of the enormity of the parameter space.  The solution is to use a smarter integration algorithm.

One way to achieve this is to perform \emph{importance sampling}.  If we can find a cheap function that approximates the posterior, we can Monte-Carlo integrate it with much fewer, more intelligently distributed samples.  There is of course no unique right way of doing this.

One approach is to try a separable importance function.  This means choosing parameters that the posterior is separable in.  The existence of ring structures on the sky maps imply that $(\tau_1, \tau_2, \tau_3)$ will be a more separable basis than $(\tau, \theta, \phi)$ (we will concentrate on the $N=3$ case for now).
\begin{eqnarray}
p(\mathbf{x}|\tau_1,\tau_2,\tau_3,\rho,H_\textrm{signal}) \approx \prod_{i=1}^3 g(\mathbf{x},\tau_i)
\end{eqnarray}

The Bayes factor can be expressed as
\begin{eqnarray}
f(\tau_2-\tau_1,\tau_3-\tau_2)
\sqrt{|\mathbf{I}-\mathbf{\Sigma K}|}\exp(\sum_{ijkl}C_{ijkl}(\tau_2-\tau_1,\tau_3-\tau_1)(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_i\mathbf{w}_j(\tau_i))(\mathbf{x}_k^T\mathbf{\Sigma}^{-1}_k\mathbf{w}_l(\tau_k)))
\end{eqnarray}
suggesting an separable importance function of the form
\begin{eqnarray}
\prod_{i=1}^3\exp(\sum_{jk} b_{ijk} (\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_i\mathbf{w}_j(\tau_i))(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_i\mathbf{w}_k(\tau_i)))
\end{eqnarray}
where we could obtain $b_{ijk}$ in some tedious fashion.  However, we can also fairly easily obtain a well-motivated expression very like the above.

Consider
\begin{eqnarray}
\frac{p(\mathbf{x}_i|\tau,\rho,H_\textrm{signal},I)}{p(\mathbf{x}_i|H_\textrm{noise},I)}&=&
\int_{-\pi}^\pi\int_0^\pi
p(\theta,\phi|H_\textrm{signal},I)
\sqrt{|\mathbf{I}-\mathbf{\Sigma}_i\mathbf{K}|}
\exp(\frac{1}{2}\mathbf{x}_i^T\mathbf{K}\mathbf{x}_i)
\mathrm{d}\theta\mathrm{d}\phi\nonumber
\end{eqnarray}
where we adopt the convention that $\tau$ is measured at the detector.  We can also adopt a polarization coordinate system such that $F_i^\times=0$, and the simplifying assumptions that
\begin{eqnarray}
\mathbf{w}_1\mathbf{\Sigma}^{-1}_i\mathbf{w}_1&=&\mathbf{w}_2\mathbf{\Sigma}^{-1}_i\mathbf{w}_2\\
\mathbf{w}_1\mathbf{\Sigma}^{-1}_i\mathbf{w}_2&=&0
\end{eqnarray}
We then get
\begin{eqnarray}
\mathbf{x}^T_i{\Sigma}^{-1}_i\mathbf{FW}&=&F_i^+\left[
\begin{array}{c}
\mathbf{x}^T_i{\Sigma}^{-1}_i\mathbf{w}_1 \\
\mathbf{x}^T_i{\Sigma}^{-1}_i\mathbf{w}_2 \\
0 \\
0
\end{array}
\right]
\end{eqnarray}
and
\begin{eqnarray}
(\mathbf{FW})^T\mathbf{\Sigma}^{-1}\mathbf{FW}&=&
(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1
\left[
\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{array}
\right]
\end{eqnarray}
and
\begin{eqnarray}
((\mathbf{FW})^T\mathbf{\Sigma}_1^{-1}\mathbf{FW}+\mathbf{A}^{-1})^{-1}&=&
\left[
\begin{array}{cccc}
\frac{1}{(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1 + \sigma^{-2}} & 0 & 0 & 0 \\
0 & \frac{1}{(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1 + \sigma^{-2}} & 0 & 0 \\
0 & 0 & \sigma^2 & 0 \\
0 & 0 & 0 & \sigma^2 \\
\end{array}
\right]
\end{eqnarray}
and
\begin{eqnarray}
\mathbf{x}_1^T\mathbf{K}\mathbf{x}&=&\frac{(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1)^2+(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_2)^2}{\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1 + (F_i^+)^{-2}\sigma^{-2}}
\end{eqnarray}
and
\begin{eqnarray}
\sqrt{|\mathbf{I}-\mathbf{\Sigma}_1\mathbf{K}|}&=&
\frac{1}{\sigma^2(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1+1}
\end{eqnarray}
%\frac{1}{\sigma^2(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1}

so we want
\begin{eqnarray}
\int_{0}^1
p(F_i^+)
\frac{1}{\sigma^2(F_i^+)^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1+1}
\exp(\frac{1}{2}\frac{(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1)^2+(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_2)^2}{\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1+(F_i^+)^{-2}\sigma^{-2}})
\mathrm{d}F_i^+
\end{eqnarray}
In the large $\mathbf{x}$ and $\sigma$ regime, the integral is dominated by the contribution from the largest $F_i^+$; in other regimes it isn't strongly affected by $F_i^+$? (Check)  So to get a crude approximation we can just set $F_i^+ = 1$; the result is the Bayesian matched filter for a signal of size $\sigma$.
\begin{eqnarray}
&\approx&
\frac{1}{\sigma^2\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1+1}
\exp(\frac{1}{2}\frac{(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1)^2+(\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_2)^2}{\mathbf{w}_1^T\mathbf{\Sigma}^{-1}_1\mathbf{w}_1+\sigma^{-2}})
\end{eqnarray}
Why not just use the matched filter expression and dispense with $\sigma$ too? Because it allows us to weight the interferometers properly.  If one of the interferometers is much less sensitive than the others, the use of $\sigma$ flattens out the importance function; we will essentially ignore that interferometer's contribution.

The straightforward Monte-Carlo integration strategy is now: draw $\tau_i$ from the distributions defined by normalizing the above function for each $i$.  Sample the full Bayes factor function for these parameters (marginalizing over multiple directions that are consistent with a particular time, such as mirror directions for three detectors or rings for two detectors), and weight them by the inverse of the product of the sampling functions.  The average of many such samplings is the integral.  If the importance function is well chosen, it will converge much more quickly than a simple integration.

It is important to remember that, though the data contains noise, the integral we wish to perform is for a particular noise realization and is entirely deterministic.  There is no notion of losing degrees of freedom or introducing correlations when we use functions of the data to recast the integral like this.  However, the fact we are dealing with noise realizations will put a limit on the useful accuracy to compute the integral to: we want the integrator to contribute less uncertainty to the result than the uncertainty due to the instrumental noise.

The vast majority of $\tau$ triplets are invalid (no sky positions correspond to them) with the integral zero there.  This isn't a problem (the integral is zero there), but we must make a special effort to sample the the valid region.
The valid region is cylindrical, enclosing the $\tau_1=\tau_2=\tau_3$ diagonal, but only a very small part of parameter space.  (the observation is tens is seconds long, but the samples must be within tens of milliseconds of each other, so roughly one in a million samples will be valid if uniformly distributed). To sample the separable distribution only within this cylinder is not trivial.  It is not practical to discard samples outside, since the interior is a tiny fraction of the whole volume.

Instead, we must make new distributions, with marginalization:
\begin{eqnarray}
p(\tau_1)&=&g(\tau_1)\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} g(\tau_2) g(\tau_3) h(\tau_1,\tau_2,\tau_3) d\tau_2 d\tau_3\\
p(\tau_1)&=&g(\tau_1)\int_{\tau_1-\delta_{12}}^{\tau_1+\delta_{12}}g(\tau_2)\int_{\tau_1-\delta_{13}}^{\tau_1+\delta_{13}}g(\tau_3)h(\tau_1,\tau_2,\tau_3)d\tau_3d\tau_2
\end{eqnarray}
If we throw away the detail of the cylinder described by $h$,
\begin{eqnarray}
p(\tau_1)&=&g(\tau_1)\int_{\tau_1-\delta_{12}}^{\tau_1+\delta_{12}}g(\tau_2)d\tau_2\int_{\tau_1-\delta_{13}}^{\tau_1+\delta_{13}}g(\tau_3)d\tau_3
\end{eqnarray}
If we sample $\tau_1$ from this distribution, and $\tau_2, \tau_3$ from their distributions constrained around $\tau_1$, the result will be identically distributed inside the valid region to the original distribution.  Some samples (about half?) will still lie outside the valid region, but this is more acceptable.  The use of cumulative sums makes it quite efficient to compute all the above.

Once we have a set of $\tau_i$ that may be valid, we need to compute the posterior for each.  The first step is to consider the set of source directions compatible with the time delays.  For three sites, there are zero or two directions consistent with each time delay.  (For two sites, which we aren't considering yet, a whole ring on the sky is consistent.)  To recover the source direction, we need to find the solutions to
\begin{eqnarray}
\left[\begin{array}{c}\tau_2-\tau_1\\\tau_3-\tau_1\end{array}\right]
&=&
\left[\begin{array}{c}\mathbf{c}^T_1-\mathbf{c}^T_2\\\mathbf{c}^T_1-\mathbf{c}^T_3\end{array}\right]
\mathbf{n}
\end{eqnarray}
where $\mathbf{c}_i$ are the coordinates of the beamsplitters in Earth-Cartesian lightseconds and the direction of the source is normalized $|\mathbf{n}|=1$.  It is presumably cheaper to compute these in advance for all representable sky directions, except when the number of samples is very small.  Via the SVD of the matrix, we can compute the portion of $\mathbf{n}$ in the row space for every relative-time delay coordinate and store the results in a cell array indexed by delay times.  For each cell, we check if the norm of partial $\mathbf{n}$ is less than one, and then compute the two directions by adding an orthogonal component to achieve the normalization.  \texttt{wresponse} is then used to get the antenna patterns.  The kernel matrix and normalization are then computed and stored in the cell array.

We also need to compute the sky area factor otherwise we end up biased against sources in the plane of the network.  The appropriate weighting is simply $z^{-1}$, i.e. the inverse of the coefficient for the out-of-plane direction, unless we want to get really fancy and deal with the exact tile shapes.

The time resolution of the output of the Q-transform enforce a 95\% overlap in their envelopes, not in the enclosed oscillations.  We have to regenerate them at higher resolutions to provide decent sky coverage.

This makes it even more important to have an efficient sampler.  Unfortunately, the random sampling and lookup tables make this difficult to implement efficiently in \textsc{Matlab}, as it can't be vectorized.  We have implemented this part of the code in \emph{C}, as \texttt{randh}.  This has eliminated a substantial contributor to the cost of executing the function.  The remaining code is dominated by the costs of inner-loop, unvectorizable accesses to the lookup table. These cannot be eliminated without reimplementing the bulk of the algorithm in \emph{C}.

\subsection{Refinements and special cases}

The system is complicated and difficult to debug.  Rearranging it to make it more modular and with better-defined intermediate products will be important to making it maintainable.

The use of the individual energy functions is strongly motivated, but might not be the most robust.  It does not take much energy for the integrator to disregard everything but the peak.  Could a weaker importance function have benefits?  This can be phrased in terms of us not wanting to reduce variance, but something else.  Given the strongly asymmetric distributions involved, are we actually in the central limit theorem regime?  Should we be working with log-Gaussians or something?

It might be fairly cheap to sample only valid directions, again using the \texttt{cumsum} tables.

To generate sky maps (for whatever reason; will be good for debugging) we'll need to switch to a different and in many ways easier integrator, that importance samples on only the one parameter (t1) using the product of the importances.  This integrator seems very nice; could this be a hint that there is a better decomposition than the current one?  It might even be more amenable to variable numbers of detectors; the downside will be the inability to disregard directions.

\section{Implementation notes}
The importance function seems to be off by a square/square-root.  This has been corrected, but it is unknown if this is due to a bug in the code, a mistake in the derivation, or a mistake in reasoning.

The standard Monte-Carlo error estimates seem to be far too rosy; we need better error estimates.  The distribution of estimates of the mean doesn't seem to be remotely Gaussian until approximately 1000 samples, and is much larger than the $\sigma/\sqrt{N}$ estimate would suggest.  Can we derive an alternative error estimate, based (for example) on the posterior for the sum of  lognormal distributed variables.

\section{Glitch model}
The weight of evidence for a glitch in any given detector is the same as the importance function:

\begin{eqnarray}
\ln \frac{p(\mathbf{x}_i|\mathbf{w},\sigma,\mathbf{\Sigma}_i)}
{p(\mathbf{x}_i|\mathbf{\Sigma}_i)}
&=&
-
\ln(\sigma^2\mathbf{w}_1\mathbf{\Sigma}_1^{-1}\mathbf{w}_1+1)
+
\frac{1}{2}
(
(x_i\Sigma_i w_1)^2 + (x_i\Sigma_i w_2)^2
)
/(w_1\Sigma_iw_1+\sigma^{-2})
\end{eqnarray}

Now,

\begin{eqnarray}
&&\ln\frac{p(x|\sigma,\Sigma)}{p(x|\Sigma)}\\
&=&
\ln
\frac{
\prod_{i=1}^3(\frac{1}{2}p(x_i|\sigma,\Sigma_i)+\frac{1}{2}p(x_i|\Sigma_i))}
{\prod_{i=1}^3 p(x_i|\Sigma_i)}
\\
&=&
\ln\frac{1}{8}\prod_{i=1}^3(\frac{p(x_i|\sigma,\Sigma_i)}{p(x_i|\Sigma_i)}+1)
\\
&=&-\ln{8}+\sum_{i=1}^3\ln(\frac{p(x_i|\sigma,\Sigma_i)}{p(x_i|\Sigma_i)}+1)
\end{eqnarray}



\end{document}

However, we want to make a slightly different implementation to cope with the fact that we only have the matched filter data for finite times, and we only want to consider those finite times to draw from.

It is cheap to compute the independent matched filters
\begin{eqnarray}
\mathbf{x}_i^T\mathbf{\Sigma}^{-1}_i\mathbf{w}_j(\tau_i)
\end{eqnarray}


We have considerable information in the expressions
\begin{eqnarray}
\mathbf{x}\mathbf{\Sigma}_i^{-1}\mathbf{w}_j(\tau_i)
\end{eqnarray}
We expect that
\begin{eqnarray}
\frac{p(\mathbf{x}|\rho,\tau,\theta,\phi)}{\prod_{i=1}^N p(\mathbf{x}_1|\rho,\tau,\theta,\phi)}
\end{eqnarray}
will be a much flatter function than the Bayes factor.  If we change variables to the times of arrival $\tau_i$ we can use the product of the individual matched filter results for each detector as an importance sampling function and Monte-Carlo integrate the result.  This means we are preferentially sampling at times when there is energy in the detector.  A coincidence analysis with coherent followup is a approximation to this strategy: identify energetic times, then compute the coherent statistic for them.





&=&|\mathbf{I}-\mathbf{UDU}^T(\mathbf{UDU}^T+\mathbf{A}^{-1})^{-1}|
&=&|\mathbf{I}-(\mathbf{UDU}^T(\mathbf{UDU}^T)^{-1}+\mathbf{A}^{-1}(\mathbf{UDU}^T)^{-1})^{-1}|\\
&=&|\mathbf{I}-(\mathbf{I}+\mathbf{A}^{-1}(\mathbf{UDU}^T)^{-1})^{-1}|

