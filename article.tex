\documentclass[preprint2]{aastex}

\usepackage[colorlinks]{hyperref}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{stmaryrd} % For \shortuparrow and \shortdownarrow
\usepackage{upgreek} % For \upmu, upright mu

\newcommand{\earlywarning}{early-warning}
\newcommand{\NS}{NS}
\newcommand{\mbta}{MBTA}
\newcommand{\GW}{GW}%{{\sc gw}}
\newcommand{\EM}{EM}%{{\sc em}}
\newcommand{\GRB}{GRB}%{{\sc grb}}
\newcommand{\CBC}{CBC}%{{\sc cbc}}
\newcommand{\LIGO}{LIGO}%{{\sc ligo}}
\newcommand{\LCGT}{LCGT}%{{\sc lcgt}}
\newcommand{\GEO}{GEO600}%{{\sc geo600}}
\newcommand{\ISCO}{ISCO}%{{\sc isco}}
\newcommand{\SNR}{SNR}%{{\sc snr}}
\newcommand{\realtime}{real-time}
\newcommand{\Msun}{\ensuremath{M_{\odot}}}
\newcommand{\order}[1]{\ensuremath{\mathcal{O}[#1]}}
\newcommand{\tmpsamps}{\ensuremath{N}}
\newcommand{\numtmps}{\ensuremath{M}}
\newcommand{\numslices}{\ensuremath{S}}
% macros for number of svd basis functions
\newcommand{\SVD}{SVD}%{{\sc svd}}
\newcommand{\svdtmps}[1]{\ensuremath{L^#1}}
\newcommand{\numsvdtmps}{\svdtmps{s}}
% macros for sample points in slices
\newcommand{\slicesamps}[1]{\ensuremath{N^#1}}
\newcommand{\slicessamps}{\slicesamps{s}}
\newcommand{\fftblock}{\ensuremath{D}}
\newcommand{\resampsamps}{\ensuremath{N^\shortdownarrow,\, N^\shortuparrow}}
\newcommand{\fir}{FIR}%{{\sc fir}}
\newcommand{\fft}{FFT}%{{\sc fft}}
\newcommand{\fmax}{\ensuremath{f^0}}
\newcommand{\flops}{flop/s}
\newcommand{\gstlal}{{\tt gstlal}}
\newcommand{\gstreamer}{GStreamer}
\newcommand{\numcpus}{{600}}
\newcommand{\lloid}{LLOID}%{{\sc lloid}}
\newcommand{\TD}{TD}%{{\sc td}}
\newcommand{\FD}{FD}%{{\sc fd}}

% Macros for collapsing sizes of things
% From TUGboat, Volume 22 (2001), No. 4
% http://www.tug.org/TUGboat/tb22-4/tb72perlS.pdf
\def\clap#1{\hbox to 0pt{\hss#1\hss}}
\def\mathllap{\mathpalette\mathllapinternal}
\def\mathrlap{\mathpalette\mathrlapinternal}
\def\mathclap{\mathpalette\mathclapinternal}
\def\mathllapinternal#1#2{\llap{$\mathsurround=0pt#1{#2}$}} 
\def\mathrlapinternal#1#2{\rlap{$\mathsurround=0pt#1{#2}$}} 
\def\mathclapinternal#1#2{\clap{$\mathsurround=0pt#1{#2}$}}

\begin{document}

\title{Toward \earlywarning\ detection of gravitational waves from compact binary coalescence}

\shorttitle{Early-warning detection of \GW\ inspirals}

\author{
	Kipp Cannon\altaffilmark{1},
	Romain Cariou\altaffilmark{2},
	Adrian Chapman\altaffilmark{3},
	Mireia Crisp\'{i}n-Ortuzar\altaffilmark{4},
	Nickolas Fotopoulos\altaffilmark{3},
	Melissa Frei\altaffilmark{5},
	Chad Hanna\altaffilmark{6},
	Erin Kara\altaffilmark{7},
	Drew Keppel\altaffilmark{8,9},
	Laura Liao\altaffilmark{10},
	Stephen Privitera\altaffilmark{3},
	Antony Searle\altaffilmark{3},
	Leo Singer\altaffilmark{3}, and
	Alan Weinstein\altaffilmark{3}
}

\altaffiltext{1}{Canadian Institute for Theoretical Astrophysics, Toronto, ON, Canada}
\altaffiltext{2}{D\'{e}partement de physique, \'{E}cole Normale Sup\'{e}rieure de Cachan, Cachan, France}
\altaffiltext{3}{LIGO Laboratory, California Institute of Technology, MC 100-36, 1200 E. California Blvd., Pasadena, CA, USA; \texttt{leo.singer@ligo.org}}
\altaffiltext{4}{Facultat de F\'{i}sica, Universitat de Val\`{e}ncia, Burjassot, Spain} 
\altaffiltext{5}{University of Texas at Austin, Austin, TX, USA}
\altaffiltext{6}{Perimeter Institute for Theoretical Physics, Waterloo, ON, Canada}
\altaffiltext{7}{Department of Physics and Astronomy, Barnard College, Columbia University, New York, NY, USA}
\altaffiltext{8}{Albert-Einstein-Institut, Max-Planck-Institut f\"{u}r Gravitationphysik, Hannover, Germany}
\altaffiltext{9}{Leibniz Universit\"{a}t Hannover, Hannover, Germany}
\altaffiltext{10}{Ryerson University, Toronto, ON, Canada}

\keywords{gamma ray burst: general --- gravitational waves --- methods: data analysis --- methods: numerical}

\begin{abstract}
Rapid detection of compact binary coalescence with a network of advanced
gravitational-wave detectors will offer a unique opportunity for
multi-messenger astronomy.  Prompt detection alerts for the astronomical
community might make it possible to observe the onset of electromagnetic
emission from compact binary coalescence.  We demonstrate a computationally
practical filtering strategy that could produce early-warning triggers before
gravitational radiation from the final merger has arrived at the detectors.
\end{abstract}

\section{Introduction}

As a compact binary system loses energy to gravitational waves (\GW{}s), its
orbital separation decays, leading to a runaway inspiral with the \GW\
amplitude and frequency increasing until the system eventually merges.  If a
neutron star (\NS) is involved, it might become tidally disrupted near the
merger and fuel an electromagnetic (\EM) counterpart \citep{shibata:2007}.
Effort from both the \GW\ and the broader astronomical communities might make
it possible to use \GW\ observations as early warning triggers for \EM\
followup. In the first generation of ground-based laser interferometers, the
\GW\ community initiated a project to send alerts when potential \GW\
transients were observed in order to trigger followup observations by \EM\
telescopes.  The typical latencies were 30 minutes \citep{HugheyGWPAW2011}.
This was an important achievement, but too late to catch any prompt \EM\
emission.  Since the \GW\ signal is in principle detectable even before the tidal
disruption, one might have the ambition of reporting \GW\ candidates not minutes
after the merger, but seconds before.  We explore one essential ingredient of
this problem, a computationally inexpensive latency free, real-time filtering
algorithm for detecting inspiral signals in \GW\ data.  We also consider the
prospects for advanced \GW\ detectors and discuss other areas of work that would
be required for rapid analysis.

Compact binary coalescence (\CBC) is a plausible progenitor for most short
gamma-ray bursts (short \GRB{}s) \citep{Lee:2005, nakar07}, but the
association is not iron-clad \citep{2011ApJ...727..109V}. The tidally
disrupted material falls onto the newly formed, rapidly spinning compact object
and is accelerated in jets along the spin axis with a timescale of $0.1$--$1$~s
after the merger \citep{Janka1999}, matching the short \GRB\ duration
distribution well. Prompt \EM\ emission including the \GRB\ can arise as fast
outflowing matter collides with slower matter ejected earlier in inner shocks.
The same inner shocks, or potentially reverse shocks, can produce an
accompanying optical flash \citep{Sari99}. The prompt emission is a probe into
the extreme initial conditions of the outflow, in contrast with afterglows,
which arise in the external shock with the local medium and are relatively
insensitive to initial conditions. Optical flashes have been observed for a
handful of long \GRB{}s \citep{2011CRPhy..12..255A} by telescopes with extremely
rapid response or, in the case of GRB 080319b, by pure serendipity, where
several telescopes were already observing the afterglow of another \GRB\ in
the same field of view \citep{2008Natur.455..183R}. The observed optical flashes
peaked within tens of seconds and decayed quickly. For short \GRB\ energy
balance and plasma density, however, the reverse shock model predicts a peak
flux in radio, approximately 20 minutes after the \GRB, but also a relatively
faint optical flash \citep{nakar07}; for a once-per-year Advanced LIGO event at
$130$~Mpc, the radio flux will peak around 9~GHz at $\sim$$5$~mJy, with emission
in the $R$-band at $\sim$19~mag. Interestingly, roughly a quarter to half of
observed short \GRB{}s also exhibit extended X-ray emission of $30$--$100$~s in
duration beginning $\sim$$10$~s after the \GRB\ and carrying comparable fluence
to the initial outburst. This can be explained if the merger results in the
formation of a proto-magnetar that interacts with
ejecta~\citep{Bucciantini2011}. Rapid \GW\ alerts would enable joint \EM\ and
\GW\ observations to confirm the short \GRB-\CBC\ link and allow the early
\EM\ observation of exceptionally nearby and thus bright events.

In October 2010, \LIGO{}\footnote{\url{http://www.ligo.org/}} completed its
sixth science run (S6) and Virgo\footnote{\url{http://www.ego-gw.it/}}
completed its third science run (VSR3).  While both \LIGO\ detectors and Virgo
were operating, several all-sky detection pipelines operated in a low-latency
configuration to send astronomical alerts, namely \mbta, Coherent
WaveBurst, and Omega \citep{HugheyGWPAW2011, S6lowlatency2, S6lowlatency3, S6lowlatency4}.
\mbta\ achieved the best \GW\ trigger-generation latencies of 2--5 minutes.
Alerts were sent with latencies of 30--60 minutes, dominated by human vetting.
Candidates were sent for \EM\ followup to several telescopes; Swift,
LOFAR, ROTSE, TAROT, QUEST, SkyMapper,
Liverpool Telescope, Pi of the Sky, Zadko, and Palomar Transient Factory
\citep{kanner2008, HugheyGWPAW2011} imaged some of the most likely sky
locations.

There were a number of sources of latency associated with the search for
\CBC\ signals in S6/VSR3 \citep{HugheyGWPAW2011}, listed here.

\paragraph{Data acquisition and aggregation ($\gtrsim$100~ms)}%
The \LIGO\ data acquisition system collects data from detector subsystems 16
times a second~\citep{Bork2001}. Data are also copied from all of the \GW\
observatories to the analysis clusters over the Internet, which is capable of
high bandwidth but only modest latency.  Together, these introduce a latency of
$\gtrsim$$100$~ms.  These technical sources of latency could be reduced with
significant engineering and capital investments, but they are minor compared
to any of the other sources of latency.

\paragraph{Data conditioning ($\sim$1~min)}%
Science data must be calibrated using the detector's frequency response to
gravitational radiation.  Currently, data are calibrated in blocks of 16~s.
Within $\sim$1~min, data quality is assessed in order to create veto flags.
These are both technical sources of latency that might be addressed with
improved calibration and data quality software for advanced detectors.

\paragraph{Trigger generation (2--5~min)}%
Low-latency data analysis pipelines
deployed in S6/VSR3 achieved an impressive latency of minutes.  However, second
to the human vetting process, this dominated the latency of the entire \EM\
followup process.  Even if no other sources of latency existed, this trigger
generation latency is too long to catch prompt or even extended emission.
Low-latency trigger generation will become more challenging with advanced
detectors because inspiral signals will stay in band up to ten times longer.  In
this work, we will focus on reducing this source of latency.

\paragraph{Alert generation (2--3~min)}%
S6/VSR3 saw the introduction of low-latency astronomical alerts, which required
gathering event parameters and sky localization from the various online
analyses, downselecting the events, and calculating telescope pointings.  If
other sources of latency improve, the technical latency associated with this
infrastructure could dominate, so work should be done to improve it.

\paragraph{Human validation (10--20~min)}%
Because the new alert system was commissioned during S6/VSR3, all alerts were
subjected to quality control checks by human operators before they were
disseminated. This was by far the largest source of latency during S6/VSR3.
Hopefully, confidence in the system will grow to the point where no human
intervention is necessary before alerts are sent, so we give it no further
consideration here.

\paragraph{}

This work will focus on reducing the latency of trigger production.  Data
analysis strategies for advance detection of \CBC{}s will have to strike a
balance between latency and throughput. \CBC\ searches consist of banks of
matched filters, or cross-correlations between the data stream and a bank of
nominal ``template'' signals.  There are many different implementations of
matched filters, but most have high throughput at the cost of high latency, or
low latency at the cost of low throughput.  The former are epitomized by the
overlap-save algorithm for frequency domain (\FD) convolution, currently the
preferred method in \GW\ searches.  The most obvious example of the latter is
time domain (\TD) convolution, which is latency-free.  However, its cost in floating
point operations per second is linear in the length of the templates, so it is
prohibitively expensive for long templates.  Several efforts are under way to
develop low-latency \CBC\ search pipelines with tractable computing requirements.
Examples include \mbta{}~\citep{Marion2004, Buskulic2010}, which was deployed in
S6/VSR3, and a novel approach using networks of IIR filters that is being explored
by \citet{shaunIIR, linqingIIR}.

Fortunately, the morphology of inspiral signals can be exploited to offset some
of the computational complexity of low-latency algorithms.  First, the signals
evolve slowly in frequency, so that they can be broken into contiguous
band-limited time intervals and processed at possibly lower sample rates.
Second, inspiral filter banks consist of highly similar templates, admitting
methods such as singular value decomposition (\SVD) to reduce the number of
templates \citep{Cannon:2010p10398}. We will use both aspects to demonstrate
that a very low latency detection statistic is possible with current computing
resources.  Assuming the other technical sources of latency can be reduced
significantly, this could make it possible to send prompt alerts to the
astronomical community.

The paper is organized as follows.  First, we discuss prospects for early-warning
detection.  Then, we provide an overview of our novel method for detecting compact
binary coalescence signals near real-time. We then describe a prototype
implementation using open source signal processing software.  To validate our approach
we present results of simulations.  We conclude with some remarks on what remains to
prepare for the advanced detector era.

\section{Prospects for \earlywarning\ detection and \EM\ followup}

\begin{figure}[h]
\plotone{f1}
\caption{\label{fig:earlywarning}Expected number of \NS--\NS\
sources that could be detectable by Advanced \LIGO\ a given number of seconds
before coalescence.  The heavy solid line is the most realistic yearly rate
estimate.  The shaded region represents the 5\% to 95\% confidence interval
arising from uncertainty in predicted event rates \citep{Abadie:2010p10836}.}
\end{figure}
%
Before the \GW\ signal leaves the detection band, we can imagine examining the
signal to noise ratio (\SNR) accumulated up to that point and if it is
already significant, release an alert immediately, trading \SNR\ and sky
localization accuracy for pre-merger detection.
Figure~\ref{fig:earlywarning} shows projected early detectability rates for
\NS--\NS\ binaries in Advanced \LIGO\ assuming event rates
predicted in \citet{Abadie:2010p10836} and anticipated detector sensitivity for
the `zero detuning, high power' configuration described in \citet{ALIGONoise}.
The most realistic estimates indicate that at an \SNR\ threshold of 8, we will
observe a total of 40~events~yr$^{-1}$. $\sim$10~yr$^{-1}$ will be detectable
within 10~s of merger and $\sim$5~yr$^{-1}$ will be detectable within 25~s of
merger if analysis can proceed with near zero latency. These rates are shown
with their substantial uncertainties in Figure~\ref{fig:earlywarning}.

We emphasize that any practical \GW\ search will include technical delays due
to light travel time between the detectors, detector infrastructure, and the
selected data analysis strategy.  Figure~\ref{fig:earlywarning} must be understood
in the context of all of the potential sources of latency, some of which are avoidable
and some of which are not. 


\begin{table}[h]
\caption{\label{table:sky-localization-accuracy}Horizon distance, \SNR\ at
merger, and area of 90\% confidence at selected times before merger for sources
with expected detectability rates of 40, 10, 1, and 0.1~yr$^{-1}$.}
\begin{center}
\begin{tabular}{rrrrrrr}
\tableline\tableline
rate & horiz. & final & \multicolumn{4}{c}{$A$(90\%) (deg$^2$)} \\
\cline{4-7}
yr$^{-1}$ & (Mpc) & \SNR\ & 25 s & 10 s & 1 s & 0 s \\
\tableline
40\phd\phn & 445 & 8.0 & 15500 & 3100 & 200 & 9.6 \\
10\phd\phn & 280 & 12.7 & 6100 & 1200 & 78 & 3.8 \\
1\phd\phn & 130 & 27.4 & 1300 & 260 & 17 & 0.8 \\
0.1 & 60 & 58.9 & 280 & 56 & 3.6 & 0.2 \\
\tableline
\end{tabular}
\end{center}
\end{table}
%
\begin{figure}[h]
\plotone{f2}
\caption{\label{fig:sky-localization-accuracy}Area of the 90\% confidence
region as a function of time before coalescence for sources with anticipated
detectability rates of 40, 10, 1, and 0.1~yr$^{-1}$. The heavy dot indicates
the time at which the accumulated \SNR\ exceeds a threshold of~8.}
\end{figure}

\EM\ followup requires estimating the location of the \GW\ source. The localization
uncertainty can be estimated from the uncertainty in the time of arrival of the \GW{}s,
which is determined by the signal's effective bandwidth and \SNR\
\citep{Fairhurst2009}.  Table~\ref{fig:earlywarning} and
Figure~\ref{fig:sky-localization-accuracy} show the estimated minimum 90\%
confidence area versus time of the loudest coalescence events detectable by
Advanced \LIGO\ and Advanced Virgo.  Once per year, we expect to observe an
event with a final \SNR\ of $\approx$27 whose location can be constrained to about
1300~deg$^2$ (3.1\% of the sky) within 25~s of merger,
260~deg$^2$ (0.63\% of the sky) within 10~s of merger, and
0.82~deg$^2$ (0.0020\% of the sky) at merger.

It is unfeasible to search hundreds of square degrees for a prompt counterpart.
For reference, LSST should see first light around the same time as
Advanced \LIGO\ and will have an unparalleled field of view of 9.6~deg$^2$
\citep{2008arXiv0805.2366I}.  However, 
it is possible to reduce the localization uncertainty by only looking at
galaxies from a catalog that lie near the sky location and luminosity distance
estimate from the \GW\ signal~\citep{galaxy-catalog} as was done in S6/VSR3.
Within the expected Advanced \LIGO\ \NS--\NS\ horizon distance,
the number of galaxies that can produce a given signal amplitude is much larger
than in Initial \LIGO\ and thus the catalog will not be as useful
for downselecting pointings for most events. However, exceptional \GW\ sources will
necessarily be extremely nearby. Within this reduced volume there will be fewer
galaxies to consider for a given candidate and catalog completeness will be
less of a concern.  This should reduce the 90\% confidence area substantially.
Proposed third generation \GW\ detectors would improve the \SNR\ and likewise
sky localization.

\section{Novel \realtime\ algorithm for \CBC\ detection}
\label{sec:method}

In this section we describe a decomposition of the \CBC\ signal space that
reduces \TD\ filtering cost sufficiently to allow for the
possibility of \earlywarning\ detection with modest computing requirements.  We
expand on the ideas of \citet{Marion2004} and \citet{Buskulic2010} that describe a
multi-band decomposition of the compact binary signal space that resulted in
a search with minutes latency during S6/VSR3~\citep{HugheyGWPAW2011}.  We combine this
with the \SVD\ rank-reduction method of \citet{Cannon:2010p10398} that exploits
the redundancy of the template banks.

\subsection{Conventional \CBC\ searches}

Inspiral signals are continuously parameterized by a set of intrinsic source
parameters $\theta$ that determine the amplitude and phase evolution of the
\GW\ strain. For systems in which the effects of spin can be ignored, the intrinsic
source parameters are just the component masses of the binary,
 $\theta = (m_1, m_2)$. For a given source, the strain observed by the
 detector is a linear combination of two waveforms corresponding to the
`$+$' and `$\times$' \GW\ polarizations.  Thus, we must design two filters
for each $\theta$.

Searches for inspiral signals typically employ matched filter
banks that discretely sample the possible intrinsic parameters~\citep{findchirppaper}.
The coefficients for the $\numtmps$ filters are known as templates, 
and are formed by discretizing and time reversing the
waveforms and weighting them by the inverse amplitude spectral density of the
detector's noise.
To construct a template bank, templates are chosen with
$\numtmps/2$ discrete signal parameters $\theta_0,\, \theta_1,\, \dots,\,
\theta_{\numtmps/2-1}$. These are chosen such that any possible signal
will have an inner product $\geqslant$0.97 with at least one template.
Such a template bank is said to have a {\em minimal match} of 0.97~\citep{Owen:1998dk}.

Filtering the detector data involves a convolution of the data with the
templates.  For a unit-normalized template $h_i[k]$ and whitened detector data
$x[k]$, both sampled at a rate $f^0$, the result can be interpreted as the
\SNR, $\rho_i[k]$ defined as
%
% Filtering equations
%
\begin{equation}
	\label{eq:SNRTD}
	\rho_i [k] = \sum_{n=0}^{N-1} h_{i}[n] x [k-n].
\end{equation}
This results in $\numtmps$ \SNR\ time series. Local peak-finding across time and
template indices results in single-detector triggers.  Coincidences are sought
between triggers in different \GW\ detectors in order to form detection candidates.

Equation~(\ref{eq:SNRTD}) can be implemented in the time-domain as a bank of
finite impulse response (\fir) filters, requiring $\mathcal O(\numtmps
\tmpsamps)$ floating point operations per sample.  However, it is typically
much more computationally efficient to use the convolution theorem and the
Fast Fourier Transform (\fft) to implement fast convolution in the frequency
domain, requiring only
$\mathcal O(\numtmps \lg \tmpsamps)$ operations per sample but incurring
a latency of $\mathcal O(\tmpsamps)$ samples.


\subsection{The \lloid\ method}

Here we describe a method for reducing the computational cost of a \TD\ search
for compact binary coalescence.  We give a zero latency, real-time algorithm
that competes in terms of floating point operations per second with the
conventional \FD\ method, which by contrast requires a significant latency due
to the inherent acausality of the Fourier transform.  Our method, called \lloid\
(Low Latency Online Inspiral Detection), involves two transformations of the
templates that produce a network of orthogonal filters that is far more
computationally efficient than the original bank of matched filters.

The first transformation is to chop the templates into disjointly supported
intervals, or \emph{time slices}.  Since the time slices of a given template
are disjoint in time, they are orthogonal with respect to time.  Given the
chirp-like structure of the templates, the ``early'' (lowest frequency) time
slices have significantly lower bandwidth and can be safely downsampled.
Downsampling reduces the total number of filter coefficients by a factor of
$\sim$100 by treating the earliest part of the waveform at $\sim$$1/100$ of
the full sample rate.  Together, the factor of 100 reduction in the number of
filter coefficients and the factor of 100 reduction in the sample rate save a
factor of $\sim$$10^4$ floating point operations per second (\flops) over the
original (full sample rate) templates.

However, the resulting filters are still not
orthogonal across the parameter space, and are in fact highly redundant.
We use the \SVD\ to approximate the template bank by a set of orthogonal
\emph{basis filters}~\citep{Cannon:2010p10398}.  We find that this approximation
reduces the number of filters needed by another factor of $\sim$100.  These two
transformations combined reduce the number of floating point operations
to the level that is competitive with the conventional high-latency \FD\
matched filter approach.  In the remainder of this section we describe the
\lloid\ algorithm in detail and provide some basic computational cost scaling.

\subsubsection{Selectively reducing the sample rate of the data and templates}
\label{sec:time-slices}

The first step of our proposed method is to divide the templates into time
slices in a \TD\ analogue to the \FD\ decomposition described
in \citet{Marion2004} and \citet{Buskulic2010}.  We decompose each template
$h_{i}[k]$ into a sum of $S$ non-overlapping templates
%
\begin{equation}
\label{eq:time-slices}
h_{i}[k] = \sum_{s=0}^{S-1}
	\begin{cases}
		h_i^s[k] & \textrm{if } t^s \leqslant k / f^0 < t^{s+1} \\
		0 & \textrm{otherwise}
	\end{cases}
\end{equation}
%
for $S$ integers $\{f^0 t^s\}$ such that $0  = f^0 t^0 < f^0 t^1 < \cdots < f^0
t^S = N$.  The outputs of these new time-sliced filters
form an ensemble of partial \SNR\ streams.  By linearity of the filtering
process, these partial \SNR\ streams can be summed to reproduce the
\SNR\ of the full template.

Since waveforms with neighboring intrinsic source parameters $\theta$
 have similar time-frequency evolution, it is possible to design computationally
efficient time slices for an extended region of parameter space rather than to
design different time slices for each template.

For concreteness and simplicity, consider an inspiral waveform in the
quadrapole approximation, for which the time-frequency relation~\citep{kidder1992, findchirppaper} is
%
\begin{equation} \label{eq:fgw}
%
f(t) = \frac{1}{\pi \mathcal{M}} \left[ \frac{5}{256}\frac{\mathcal{M}}{t}
\right]^{3/8}.
%
\end{equation}
%
Here, $\mathcal{M}$ is the chirp mass of the binary in units of time (where $G
M_\odot / c^3 \approx 5$~$\upmu$s) and $t$ is the time relative to the
coalescence of the binary.
This monotonic time-frequency relationship allows us
to choose time slice boundaries that require substantially less bandwidth at
early times in the inspiral.

An inspiral signal will enter the detection band with some low frequency
$f_\mathrm{low}$ at time $t_\mathrm{low}$ before merger.  Usually the template
is truncated at some prescribed time $t^0$, or equivalent frequency $f_\mathrm{high}$,
often chosen to correspond to the \ISCO. The beginning of the template is critically
sampled at $2 f_\mathrm{low}$, but the end of the template is critically sampled at a
rate of $2 f_\mathrm{high}$. In any time interval smaller than the duration of the template,
the bandwidth of the filters across the entire template bank can be significantly less
than the full sample rate at which data is acquired.

Our goal is to reduce the filtering cost of a
large fraction of the waveform by computing part of the convolution at a lower
sample rate.  Specifically we consider here time slice boundaries with the
smallest power-of-two sample rates that sub-critically sample the time-sliced
templates.  The time slices consist of the $S$ intervals
$\left[t^0, t^1\right),\, \left[t^1, t^2\right),\, \dots,\, \left[t^{S-1}, t^S\right)$,
sampled at frequencies $f^0,\, f^1,\, \dots,\, f^\mathrm{S-1}$, where $f^s$ is at
least twice the highest nonzero frequency component of any filter in the bank for the
$s$th time slice.

The time sliced templates can then be downsampled in each interval without
aliasing, so we define them as
%
\begin{equation}
\label{eq:time-sliced-templates}
h_{i}^{s}[k] \equiv
	\begin{cases}
		h_{i}\!\left[k\frac{f}{f^s}\right] & \textrm{if } t^s \leqslant k/f^s < t^{s+1} \\
		0 & \textrm{otherwise.}
	\end{cases}
\end{equation}
%
We note that the time slice decomposition in equation~(\ref{eq:time-slices}) is
manifestly orthogonal since the time slices are disjoint in time.  In the next
section we examine how to reduce the number of filters within each time slice
via \SVD\ of the time-sliced templates.

\subsubsection{Reducing the number of filters with the \SVD}
\label{sec:svd}

As noted previously, the template banks used in inspiral searches are by design
highly correlated.  \citet{Cannon:2010p10398} showed that applying the \SVD\
to inspiral template banks greatly reduces the number of filters required to achieve a
particular minimal match.  A similar technique can be applied to the time-sliced
templates as defined in equation~\ref{eq:time-sliced-templates} above.  The \SVD\
is a matrix factorization that takes the form
%
\begin{equation}
h_i^s[k] = \sum_{\mathclap{l=0}}^{\mathclap{M-1}} v_{il}^s \sigma_l^s u_l^s[k] \approx \sum_{\mathclap{l=0}}^{\mathclap{L^s-1}} v_{il}^s \sigma_l^s u_l^s[k].
\label{eq:svddecomp}
\end{equation}
where $u_l^s[k]$ are orthonormal \emph{basis templates} related to the original
time-sliced templates through the \emph{reconstruction matrix}, $v_{il}^s\sigma_l^s$.
The expectation value of the fractional loss in \SNR\ is the \SVD\ tolerance, given by
%
\begin{equation*}
\left[ \sum_{l=0}^{L^s-1} \left( \sigma_l^s \right)^2 \right]\left[ \sum_{l=0}^{M-1} \left( \sigma_l^s \right)^2 \right]^{-1},
\end{equation*}
%
determined by the number $\numsvdtmps$ of basis templates that are kept in
the approximation.  The authors of \citet{Cannon:2010p10398}
showed that highly accurate approximations of inspiral template banks could be
achieved with few basis templates.  We find that when combined with the
time slice decomposition, the number of basis templates \numsvdtmps\ is much
smaller than the original number of templates \numtmps\ and improves on the
rank reduction demonstrated in \citep{Cannon:2010p10398} by nearly an order
of magnitude.

Because the sets of filters from each time slice form orthogonal subspaces, and
the basis filters within a given time slice are mutually orthogonal, the set of
all basis filters from all time slices forms an orthogonal basis spanning the
original templates.

In the next section we describe how we form our early-warning detection
statistic using the time slice decomposition and the \SVD.

\subsubsection{Early-warning output}

In the previous two sections, we described two transformations that greatly
reduce the computational burden of \TD\ filtering.  We are now prepared to
define our detection statistic, the early-warning output, and to comment on the
computational cost of evaluating it.

First, the sample rate of the detector data must be decimated to match sample
rates with each of the time slices.  We will denote the decimated detector data
streams using a superscript $^s$ to indicate the time slices to which they
correspond.  The operator $H^\shortdownarrow$ will represent the appropriate
decimation filter that converts between the base sample rate $f^0$ and the
reduced sample rate $f^s$:
\begin{equation*}
\label{eq:decomp}
	x^{s}[k] = \left( H^\shortdownarrow x^0\right)[k].
\end{equation*}
We shall use the symbol $H^\shortuparrow$ to represent an interpolation filter
that converts between sample rates $f^{s+1}$ and $f^s$ of adjacent time slices,
\begin{equation*}
	x^{s}[k] = \left( H^\shortuparrow x^{s+1}\right)[k].
\end{equation*}
Ultimately, the latency of the entire \lloid\ algorithm is set by the decimation
and interpolation filters because they are generally time-symmetric and slightly
acausal.  As long as the latency introduced by the decimation and interpolation
filters for any time slice $s$ is less than that time slice's delay $t^s$, the
total latency of the \lloid\ algorithm will be zero.

From the combination of the time slice decomposition in
equation~(\ref{eq:time-sliced-templates}) and the \SVD\ defined in
equation~(\ref{eq:svddecomp}), we define the early-warning output accumulated
up to time slice $s$ using the recursion relation,
%
% orthogonal decomposition filtering
%
\begin{multline}
	\rho_i^s [k] =%
		% Interpolation SNR
		\overbrace{
			\left(H^\uparrow \rho_i^{s+1}\right)[k]
		}^\textrm{\clap{{\sc snr} from previous time slices}} \\
		% Plus ...
		+
		% Reconstruction
		\underbrace{
			\sum_{\mathclap{l=0}}^{\mathclap{L^s-1}} v_{il}^s \sigma_l^s
		}_\textrm{\clap{reconstruction}}
		% Orthogonal FIR filter
		\overbrace{
			\sum_{\mathclap{n=0}}^{\mathclap{N^s-1}} u_l^s[n] x^s[k-n]
		}^\textrm{\clap{orthogonal {\sc fir} filters}} .
\end{multline}
%
%
Observe that the early-warning output for time slice 0, $\rho_i^0[k]$,
approximates the \SNR\ of the original templates.
%
%
\begin{figure*}[h!]
	\begin{center}
		\includegraphics{f3}
		\caption{\label{fig:pipeline} Schematic of \lloid\ pipeline illustrating
signal flow.  Circles with arrows represent interpolation
\protect\includegraphics{f3a} or decimation
\protect\includegraphics{f3b}.  Circles with plus
signs represent summing junctions
\protect\includegraphics{f3c}.  Squares
\protect\includegraphics{f3d} stand for \fir\ filters.  Sample
rate decreases from the top of the diagram to the bottom.  In this diagram each
time slice contains three \fir\ filters that are linearly combined to produce
four output channels.  In a typical pipeline the number of \fir\ filters is
much less than the number of output channels.}
	\end{center}
\end{figure*}
%
%
The signal flow diagram in Figure~\ref{fig:pipeline} illustrates this
recursion relation as a multirate filter network with a number of early-warning outputs.

In the next section we compute the expected computational cost scaling of this
decomposition and compare it with the brute-force \TD\ implementation of
\eqref{eq:SNRTD} and higher latency \FD\ methods.

\subsection{Comparison of computational costs}

We now examine the computational cost scaling of the conventional \TD\ or
\FD\ matched filter procedure as compared with \lloid.  For convenience,
Table~\ref{tab:recap} provides a review of the notation that we will need in
this section.
%
% symbols used in FLOPs calculations table
%
\begin{table}
\caption{\label{tab:recap}Notation used to describe filters.}
\begin{center}
\begin{tabular}{ll}
\tableline\tableline
& Definition \\
\tableline
$f^s$		& sample rate in time slice $s$ \\
\numtmps		& num. templates \\
\tmpsamps	& num. samples per template \\
\numslices	& num. time slices \\
\numsvdtmps	& num. basis templates in time slice $s$ \\
\slicessamps	& num. samples in decimated time slice $s$\\
$N^\shortdownarrow$ & length of decimation filter \\
$N^\shortuparrow$ & length of interpolation filter \\
\tableline
\end{tabular}
\end{center}
\end{table}


\subsubsection{Conventional \TD\ method}

The conventional \TD\ method consists of a bank of \fir\ filters, or
sliding-window dot products.  If there are $\numtmps$ templates, each
$\tmpsamps$ samples in length, then each filter requires $M N$ multiplications
and additions per sample, or $2 \numtmps \tmpsamps f^0$ \flops\ at a sample rate
$f^0$.

\subsubsection{Conventional \FD\ method}

The most common \FD\ method is known as the \emph{overlap-save} algorithm, described in
\citet{numerical-recipes-chapter-13}.  It entails splitting the input into blocks of $D$
samples, $D > \tmpsamps$, each block overlapping the previous one by $D - \tmpsamps$
samples.  For each block, the algorithm computes the forward \fft\ of the data and
each of the templates, multiplies them, and then computes the reverse \fft.

Modern implementations of the \fft, such as the ubiquitous \texttt{fftw}, require about
$2 \fftblock \lg \fftblock$ operations to evaluate a real transform of size
$\fftblock$~\citep{Johnson:2007p9654}.  Including the forward transform of the data and
$M$ reverse transforms for each of the templates, the \fft\ costs $2 (\numtmps + 1)
\fftblock \lg \fftblock$ operations per block.  The multiplication of the transforms adds
a further $2 \numtmps \fftblock$ operations per block.  Since each block produces
$\fftblock - \tmpsamps$ usable samples of output, the overlap-save method requires
$$
f^0 \cdot \frac{2 (\numtmps + 1) \lg \fftblock + 2 \numtmps}{1 - \tmpsamps/\fftblock} \; \mathrm{\flops} \,.
$$

In the limit of many templates, $M \gg 1$, we can neglect the cost of the forward
transform of the data and of the multiplication of the transforms.  The computational
cost will reach an optimum at some large but finite \fft\ block size
$\fftblock \gg \tmpsamps$.  In this limit, the \FD\ method costs
$\approx 2 f^0 \numtmps \lg \fftblock$ \flops.

By adjusting the \fft\ block size, it is possible to achieve low latency with FD
convolution, but the computational cost grows rapidly as the latency in samples
$(D-N$) decreases.  It is easy to show that in the limit of many templates and
long templates, $M, \lg N \gg 1$, the computational cost scales as
%
$$
\left(1 + \frac{\textrm{template length}}{\textrm{latency}}\right) \left( 2 f^0 M \lg N \right).
$$

\subsubsection{\label{sec:lloid-method}\lloid\ method}

For time slice $s$, the \lloid\ method requires $2 \slicessamps \numsvdtmps f^s$ \flops\ 
to evaluate the orthogonal filters, $2 \numtmps \numsvdtmps f^s$ \flops\ to
apply the  linear transformation from the $\numsvdtmps$ basis templates to the
$\numtmps$ time-sliced templates, and $\numtmps f^s$ \flops\ to add the
resultant partial \SNR\ stream.

The computational cost of the decimation of the detector data is a little bit
more subtle.  Decimation is achieved by applying an \fir\ anti-aliasing filter
and then downsampling, or deleting samples in order to reduce the sample rate
from $f^{s-1}$ to $f^s$.  Naively, an anti-aliasing filter with
$(f^{s-1} / f^s) N^\shortdownarrow$ coefficients should demand
$2 N^\shortdownarrow (f^{s-1})^2 / f^s$ \flops.  However, it is necessary to
evaluate the anti-aliasing filter only for the fraction $f^s / f^{s-1}$ of the
samples that will not be deleted.  Consequently, an efficient decimator that
requires only $2 N^\shortdownarrow f^{s-1}$ \flops\ is possible.

The story is similar for the interpolation filters used to match the sample
rates of the partial \SNR\ streams.  Interpolation of a data stream from a
sample rate $f^s$ to $f^{s-1}$ consists of inserting zeros between the samples
of the original stream, and then applying a low-pass filter with
$(f^{s-1} / f^s) N^\shortdownarrow$ coefficients.  The low-pass filter requires
$2 M N^\shortdownarrow (f^{s-1})^2 / f^s$ \flops.  However, by taking advantage
of the fact that by construction a fraction $f^s / f^{s-1}$ of the samples are
zero, it is possible to build an efficient interpolator that requires only
$2 M N^\shortuparrow f^{s-1}$ \flops.

Taking into account the decimation of the detector data, the orthogonal \fir\
filters, the reconstruction of the time-sliced templates, the interpolation of
\SNR\ from previous time slices, and the accumulation of \SNR, in total the
\lloid\ algorithm requires
%
\begin{multline}
\label{eq:lloid-flops}
\sum_{\mathclap{s=0}}^{\mathclap{S-1}} \left( 2 \slicessamps \numsvdtmps + 2 \numtmps \numsvdtmps + \numtmps \right) f^s \\ + 2\sum_{\mathclap{f^s \in \{f^k \, : \, 0 < k < S\}}} \left( N^\shortdownarrow f^0 + \numtmps N^\shortuparrow f^{s-1}\right)
\end{multline}
%
\flops.  The second sum is carried out over the set of distinct sample rates
(except for the base sample rate) rather than over the time slices themselves,
as we have found that it is sometimes desirable to place multiple adjacent time
slices at the same sample rate in order to keep the size of matrices that enter
the singular value decomposition manageable.  Here we have assumed that the
decimation filters are connected in parallel, converting from the base sample
rate $f^0$ to each of the time slice sample rates $f^1$, $f^2$, $\dots$, and
that the interpolation filters are connected in cascade fashion with each
interpolation filter stepping from the sample rate of one time slice to the
next.

We can simplify this expression quite a bit by taking some limits that arise
from sensible filter design.  In the limit of many templates, the cost of the
decimation filters is negligible as compared to the cost of the interpolation
filters.  Typically, we will design the interpolation filters with
$N^\shortuparrow \lesssim \numsvdtmps$ so that the interpolation cost itself is
negligible compared with the reconstruction cost.  Finally, if the number of
basis templates per time slices $\numsvdtmps$ is not too small, the
reconstruction cost dominates over the cost of accumulating the partial \SNR.
In these limits, the cost of \lloid\ is dominated by the basis filters
themselves and the reconstruction, totaling
$2 \sum_{s=0}^{S-1} f^s \numsvdtmps \left( \slicessamps + \numtmps \right)$ \flops.

\subsubsection{Speedup of \lloid\ relative to \TD\ method}

If the cost of the basis filters dominates, and the frequency of the templates
evolves slowly enough in time, then we can use the time-frequency relationship
of equation~(\ref{eq:fgw}) to estimate the speedup relative to the conventional
\TD\ method.  The reduction in \flops\ is approximately
%
\begin{multline}
\label{eq:speedup}
\frac{2 \sum_{s=0}^{S-1} f^s \numsvdtmps \slicessamps}{2 \numtmps \tmpsamps f^0} \\
\approx \frac{\alpha}{\left(t_\mathrm{low} - t_\mathrm{high}\right) \left(f^0\right)^2} \int_{t_\mathrm{low}}^{t_\mathrm{high}} \left(2 f(t) \right)^2 \, \mathrm{d} t \\
= \frac{16 \alpha \left(t_\mathrm{low} f^2 (t_\mathrm{low}) - t_\mathrm{high} f^2 (t_\mathrm{high}) \right)}{\left(f^0\right)^2 \left(t_\mathrm{low} - t_\mathrm{high}\right)}
\end{multline}
%
where $\alpha \approx \numsvdtmps / \numtmps$ is the rank reduction factor, or
ratio between the number of basis templates and the number of templates.  This
approximation assumes that the frequency of the signal is evolving very slowly
so that we can approximate the time slice sample rate as twice the instantaneous
gravitational wave frequency, $f^s \approx 2 f(t)$, and the number of samples in
the decimated time slice as the sample rate times an infinitesimally short time
interval, $\slicessamps \approx 2 f(t) \, \mathrm{d}t$. The integral is
evaluated using the power-law form of $f(t)$ from equation~(\ref{eq:fgw}).
Substituting approximate values for a template bank designed for component
masses around (1.4, 1.4) $M_\odot$, $\alpha \approx 10^{-2}$, $t_\mathrm{low} = 10^3$~s,
$f_\mathrm{low} = 10^1$~Hz, $t_\mathrm{high} = 1/1570$~s, $f_\mathrm{high} = 1570$~Hz,
and $f^0 = (2)(1570)$~Hz, we find from equation~(\ref{eq:speedup}) that the
\lloid\ method requires only $\sim 10^{-6}$ times as many \flops\ as the
conventional \TD\ method.

\section{Implementation}

In this section we describe an implementation of the \lloid\ method described
in section \ref{sec:method} suitable for rapid \GW\
searches for \CBC{}s.  The \lloid\ method requires several
computations that can be completed before the analysis is underway.  Thus
we divide the procedure into an offline planning stage and an
online, low-latency filtering stage.  The offline stage can be done before the
analysis is started and updated asynchronously, whereas the online stage must
keep up with the detector output and produce search results as rapidly as
possible.  In the next two subsections we describe what these stages entail.

\subsection{Planning stage}

The planning stage begins with choosing templates that cover the space of
source parameters with a hexagonal grid~\citep{PhysRevD.76.102004} in order to
satisfy a minimal match criterion.  This assures a prescribed maximum loss in
\SNR\ for signals whose parameters do not lie on the hexagonal grid.  Next, the
grid is partitioned into groups of neighbors called \emph{sub-banks} that
are appropriately sized so that each sub-bank can be efficiently handled by a
single computer.  Each sub-bank contains templates of comparable chirp mass, and
therefore similar time-frequency evolution.  Dividing the source
parameter space into smaller sub-banks also reduces the offline cost of the
\SVD\ and is the approach considered in \citet{Cannon:2010p10398}.  Next, we choose
time-slice boundaries as in equation~\eqref{eq:time-sliced-templates} such that all
of the templates within a sub-bank are sub-critically sampled at progressively lower
sample rates.  For each time slice, the templates are downsampled to the
appropriate sample rate.  Finally, the \SVD\ is applied to each time slice in
the sub-bank in order to produce a set of orthonormal basis templates and a
reconstruction matrix that maps them back to the original templates as
described in equation~\eqref{eq:svddecomp}.  The downsampled basis templates,
the reconstruction matrix, and the time-slice boundaries are all saved to disk.

\subsection{Filtering stage}

The \lloid\ algorithm is amenable to latency free, real-time implementation.  However, such a real-time search pipeline would require integration directly into the data acquisition and storage systems of the \LIGO\ observatories.  A slightly more 
modest goal is to leverage existing low-latency, but not real-time, signal processing software in order to implement
the \lloid\ algorithm.

We have implemented a prototype of the low-latency filtering stage using an
open-source signal processing environment called
\gstreamer\footnote{\url{http://gstreamer.net/}}.
\gstreamer\ is a vital component of many Linux systems, providing media
playback, authoring, and streaming on devices from cell phones to desktop
computers to streaming media servers.  Given the similarities of
\GW\ detector data to audio data it is not surprising that
\gstreamer\ is useful for our purpose. \gstreamer\ also provides some useful
stock signal processing elements such as resamplers and filters.  We have
extended the \gstreamer\ framework by developing a library called
\gstlal\footnote{\url{https://www.lsc-group.phys.uwm.edu/daswg/projects/gstlal.html}}
that provides elements for \GW\ data analysis.

\gstreamer\ pipelines typically operate with very low (in some consumer
applications, imperceptibly low) latency rather than in true real-time because
signals are partitioned into blocks of samples, or \emph{buffers}.  This affords
a number of advantages, including amortizing the overhead of passing signals
between elements and grouping together sequences of similar operations.
However, buffering a signal incurs a latency of up to one buffer length.  This
latency can be made small at the cost of some additional overhead by making the
buffers sufficiently small.

Our prototype pipeline, tested with GStreamer version 0.10.33, consists of the
following stages.

\paragraph{Decimation}

The whitened detector data is reduced to successively lower sample rates by
decimation. Decimation involves applying an anti-aliasing filter to the data,
and then down-sampling by deleting samples.  We use GStreamer's stock
\texttt{audioresample} element, which provides a FIR decimation filter with
a Kaiser-windowed sinc function kernel.

The detector data is provided at every power-of-two sample rate
required by the time slices described in \eqref{eq:time-slices}.  At this point,
the \gstreamer\ pipeline splits into several parallel branches to handle each
time slice.

\paragraph{Basis filters}

Next, these decimated data streams are fed into parallel \fir\ filter banks.
The basis filters are implemented using a \gstlal\ element called
\texttt{lal\_firbank}, which provides a single input, multiple output bank of
\fir\ filters from a matrix of filter coefficients.  Several instances of
\texttt{lal\_firbank} execute in parallel to handle each of the time slices.
Filter coefficients are taken from the nonzero samples of the time slices in
equation~\eqref{eq:time-sliced-templates}, with the zero-padding effectively
accomplished by appropriate queues, or delay lines.

\paragraph{Reconstruction}

From the outputs of the \SVD\ basis filters, we form the partial \SNR\ streams
for each time  slice by multiplying by the reconstruction matrices.  This is
accomplished with the \texttt{gstlal} element \texttt{lal\_matrixmixer}.

\paragraph{Interpolation}

In order to form the early-warning output from each time slice, we have to add
the partial \SNR\ to the early-warning output from the subsequent time slices.
If the next time slice does not have the same sample rate, its output must first
be interpolated.  The \gstreamer\ element \texttt{audioresample} is used again here.  

\paragraph{\SNR\ accumulation}

The early-warning output for each time slice is formed by accumulating
interpolated early-warning output from the subsequent time slice.  This process
continues until we have worked our way to the \SNR\ of the original templates
at the full sample rate $f^0$.  In this way, the \lloid\ algorithm and this
implementation naturally leads to a simple early-warning pipeline.

\section{Results}

In this section we evaluate the accuracy of the \lloid\ algorithm using our 
\gstreamer{}-based implementation described in the previous section. We calculate
the measured \SNR\ loss due to the approximations of the \lloid\ method and our
implementation of it. Using a configuration that gives acceptable \SNR\ loss
for our chosen set of source parameters, we then compare the computational cost
in \flops\ for \TD\ method, the \FD\ method, and \lloid.

\subsection{Setup}
\label{sec:bank-setup}

We examine the performance of the \lloid\ algorithm on a small region of
compact binary parameter space centered on typical \NS--\NS\
masses.  We begin by constructing a template bank that spans component masses
from 1~to~3~$M_\odot$ using a simulated Advanced \LIGO\ noise
curve~\citep{ALIGONoise}.  Templates are truncated at 10~Hz, where the projected
sensitivity of Advanced \LIGO\ is interrupted by the ``seismic wall.''
This results in a grid of 98544 points, or
$2 \times 98544 = 197088$~templates.  Then we create sub-banks by partitioning
the parameter space by chirp mass.  Figure \ref{fig:tmpltbank} illustrates this
procedure. We concentrate on a sub-bank with 657 points with chirp masses
between 1.1955 and 1.2045~$M_\odot$, or $2 \times 657 = 1314$~templates.
\begin{figure}[h]
	\plotone{f4}
	\caption{\label{fig:tmpltbank}Source parameters selected for sub-bank used in this
case study, consisting of component masses $m_1$, $m_2$, between 1 and 3~$M_\odot$, and
chirp masses $\mathcal{M}$ between 1.1955 and 1.2045~$M_\odot$.}
\end{figure}
With this sub-bank we are able to construct an efficient time-slice decomposition
that consists of 11 time slices with sample rates between 32 and 4096 Hz summarized
in Table~\ref{tab:time_slices}.
\begin{table*}
\caption{\label{tab:time_slices} Filter design sub-bank of 1314 templates.  From
left to right, this table shows the sample rate, time interval, number of
samples, and number of orthogonal templates for each time slice.  We vary \SVD\
tolerance from $\left(1-10^{-1}\right)$ to $\left(1-10^{-6}\right)$.}
\begin{center}
\begin{minipage}[c]{0.4\textwidth}
\includegraphics{t3}
\end{minipage}
\begin{minipage}[c]{0.55\textwidth}
\begin{tabular}{rr@{,\,}lc*{6}{r}}
\tableline\tableline
\\ [-2ex]
$f^s$ & $[t^s$&$t^{s+1})$ & &\multicolumn{6}{c}{$-\log_{10}$ (1$-$\SVD\ tolerance)} \\% [1ex]
\cline{5-10}
\\[-2.5ex]
(Hz) & \multicolumn{2}{c}{(s)} & $N^s$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ \\ \tableline
4096 & [0&0.5) & 2048 & 1 & 4 & 6 & 8 & 10 & 14 \\
512 & [0.5&4.5) & 2048 & 2 & 6 & 8 & 10 & 12 & 16 \\
256 & [4.5&12.5) & 2048 & 2 & 6 & 8 & 10 & 12 & 15 \\
128 & [12.5&76.5) & 8192 & 6 & 20 & 25 & 28 & 30 & 32 \\
64 & [76.5&140.5) & 4096 & 1 & 8 & 15 & 18 & 20 & 22 \\
64 & [140.5&268.5) & 8192 & 1 & 7 & 21 & 25 & 28 & 30 \\
64 & [268.5&396.5) & 8192 & 1 & 1 & 15 & 20 & 23 & 25 \\
32 & [396.5&460.5) & 2048 & 1 & 1 & 3 & 9 & 12 & 14 \\
32 & [460.5&588.5) & 4096 & 1 & 1 & 7 & 16 & 18 & 21 \\
32 & [588.5&844.5) & 8192 & 1 & 1 & 8 & 26 & 30 & 33 \\
32 & [844.5&1100.5) & 8192 & 1 & 1 & 1 & 12 & 20 & 23 \\
\tableline
\end{tabular}
\end{minipage}
\end{center}
\end{table*}
We use this sub-bank and decomposition for the remainder of this section.

\subsection{Measured \SNR\ loss}

The \SNR\ loss is to be compared with the mismatch of 0.03 that arises from the
discreteness of template bank designed for a minimal match of 0.97.  We will consider
an acceptable target \SNR\ loss to be a factor of 10 smaller than this, that is, no more
than 0.003.

We expect two main contributions to the \SNR\ loss to arise in our
implementation of the \lloid\ algorithm.  The first is the \SNR\ loss due to
the truncation of the \SVD\ at $L^s < M$ basis templates.  As remarked upon in
\citet{Cannon:2010p10398} and section~\ref{sec:svd}, this effect is measured by
the \SVD\ tolerance.  The second comes from the limited bandwidth of the
interpolation filters used to match the sample rates of the partial \SNR\ streams.
The maximum possible bandwidth is determined by the length of the filter,
$N^\shortuparrow$.  \SNR\ loss could also arise if the combination of both the
decimation filters and the interpolation filters reduces their bandwidth
measurably, if the decimation and interpolation filters do not have perfectly uniform
phase response, or if there is an unintended subsample time delay at any stage.

To measure the accuracy of our \gstreamer\ implemention of \lloid\ including all of
the above potential sources of \SNR\ loss, we conducted impulse response tests.  The
\gstreamer\ pipeline was presented with an input consisting of a unit impulse.  By
recording the outputs, we can effectively ``play back'' the templates.  These impulse
responses will be similar, but not identical, to the original, nominal templates.
By taking the inner product between the impulses responses for each output 
channel with the corresponding nominal template, we can gauge exactly how much \SNR\
is lost due to the approximations in the \lloid\ algorithm and any of the technical
imperfections mentioned above.  We call one minus this dot product the \emph{mismatch}
relative to the nominal template.

The two adjustable parameters that affect performance and mismatch the most are
the SVD tolerance and the length of the interpolation filter.  The length of the
decimation filter affects mismatch as well, but has very little impact on
performance.

\paragraph{Effect of \SVD\ tolerance}

We studied how the \SVD\ tolerance affected \SNR\ loss by holding
$N^\shortdownarrow = N^\shortuparrow = 192$ fixed as we varied the \SVD\
tolerance from $\left(1-10^{-1}\right)$ to $\left(1-10^{-6}\right)$.  The
minimum, maximum, and median mismatch are shown as functions of \SVD\ tolerance
in Figure~\ref{fig:bw}a.  As the \SVD\ tolerance increases toward 1, the \SVD\
becomes an exact matrix factorization, but the computational cost increases as
the number of basis filters increases.  The conditions presented here are more
complicated than in the original work~\citep{Cannon:2010p10398} due to the
inclusion of the time-sliced templates and interpolation, though we still see
that the average mismatch is approximately proportional to the \SVD\ tolerance
down to $\left(1-10^{-4}\right)$.  However, as the \SVD\ tolerance becomes even
higher, the mismatch seems to saturate around $2 \times 10^{-4}$.  This could be
the effect of the interpolation, or an unintended technical imperfection that we
did not model or expect.  However, this is still an order of magnitude below our
target mismatch of 0.003.  We find that an \SVD\ tolerance of
$\left(1-10^{-4}\right)$ is adequate to achieve our target \SNR\ loss.

\paragraph{Effect of interpolation filter length}

Next, keeping the \SVD\ tolerance fixed at $\left(1-10^{-6}\right)$ and the
length of the decimation filter fixed at $N^\shortdownarrow = 192$, we studied
the impact of the length $N^\shortuparrow$ of the interpolation filter on
mismatch.  The mismatch as a function of $N^\shortuparrow$ is shown in
Figure~\ref{fig:bw}b.  The mismatch saturates at $\sim$$2 \times 10^{-4}$ with
$N^\shortuparrow = 64$.  We find that a filter length of 16 is sufficient to
meet our target mismatch of 0.003.

Having selected an SVD tolerance of $\left(1-10^{-4}\right)$ and
$N^\shortuparrow=16$, we found that we could reduce $N^\shortdownarrow$ to 48
without exceeding a maximum mismatch of~0.003.

We found that careful design of the decimation and interpolation stages made a
crucial difference in terms of computational overhead.  Connecting the
interpolation filters in cascade fashion rather than in parallel resulted in a
significant speedup.  Also, only the shortest interpolation filters that met our
maximum mismatch constraint resulted in a sub-dominant contribution to the
overall cost.  There is possibly further room for optimization beyond minimizing
$N^\shortuparrow$.  We could design custom decimation and interpolation filters,
or we could tune these filters separately for each time slice.

\begin{figure*}[b]
	\begin{minipage}[t]{0.5\textwidth}
		\begin{center}
			\includegraphics{f5a}
			(a) Mismatch versus \SVD\ tolerance
		\end{center}
	\end{minipage}
	\begin{minipage}[t]{0.5\textwidth}
		\begin{center}
			\includegraphics{f5b}
			(b) Mismatch versus $N^\shortuparrow$
		\end{center}
	\end{minipage}
	\caption{\label{fig:bw}Box-and-whisker plot of mismatch between nominal
template bank and \lloid\ measured impulse responses.  The upper and lower boundaries of
the boxes show the upper and lower quartiles; the lines in the center denote the medians.
The whiskers represent the minimum and maximum mismatch over all templates.  In 
(a) the interpolation filter length is held fixed at $N^\shortuparrow = 192$, while
the \SVD\ tolerance is varied from $\left(1-10^{-1}\right)$ to $\left(1-10^{-6}\right)$.
In (b), the \SVD\ tolerance is fixed at $\left(1-10^{-6}\right)$ while $N^\shortuparrow$
is varied from 8 to 192.}
\end{figure*}

\subsection{Lower bounds on computational cost and latency compared to other
methods}

We are now prepared to offer the estimated computational cost of filtering this
sub-bank of templates compared to other methods.  We used the results of the
previous subsections to set the \SVD\ tolerance to $\left(1-10^{-4}\right)$,
the interpolation filter length to 16, and the decimation filter length to 48.
Table~\ref{table:flops} shows the computational cost in \flops\ for the sub-bank
we described above.  For the \FD\ method, an \fft\ block size of
$\fftblock = 2 \tmpsamps$ is assumed, resulting in a latency of
$\left(\tmpsamps / f^0\right)$ seconds.  Both the \FD\ method and \lloid\ are
five orders of magnitude faster than the conventional \TD\ method.  However, the
\FD\ method has a latency of over half of an hour, whereas the \lloid\ algorithm
has no latency at all.
%
\begin{table*}
\caption{\label{table:flops}Computational cost in \flops\ and latency in seconds
of the \TD\ method, the \FD\ method, and \lloid.  Cost is given
for both the sub-bank described in section~\ref{sec:bank-setup} and a full
1--3~$M_\odot$ \NS--\NS\ search.  The last column gives the approximate number of machines per
detector required for a full Advanced LIGO \NS--\NS\ search.}
\begin{center}
\begin{tabular}{lllll}
\tableline\tableline
& \flops\ & & \flops\ & number of \\
method & (sub-bank) & latency (s) & (\NS--\NS) & machines \\[0.1em]
\tableline
time domain & $4.9\times10^{13}$ & 0 & $3.8\times10^{15}$ & $\sim$$3.8\times10^5$ \\
frequency domain & $5.2\times10^8$ & $2\times10^3$ & $5.9\times10^{10}$ & $\sim$$5.9$ \\
\lloid\ (theory) & $6.6\times10^8$ & 0 & $1.1 \times 10^{11}$ & $\sim$$11$ \\
\lloid\ (prototype) & (0.9 cores) & $0.5$ & ------------ & $\gtrsim$$10$ \\
\tableline
\end{tabular}
\end{center}
\end{table*}

\subsection{Extrapolation of computational cost to an Advanced \LIGO\ search}

Table~\ref{table:flops} shows that the \lloid\ method requires $6.6 \times 10^8$
\flops\ to cover a sub-bank comprising 657 out of the total 98544 mass pairs.
Assuming that other regions of the parameter space have similar computational
scaling, an entire single detector search for \NS--\NS\ signals in the
1--3~$M_\odot$ component mass range could be implemented with
$\sim$$10^{11}$~\flops.  Modern (ca. 2011) workstations can achieve peak
computation rates up to $\sim$$10^{11}$~\flops.  In practice, we expect that a
software implementation of \lloid\ will reach average computation rates that are
perhaps a factor 10 less than this, $\sim$$10^{10}$~\flops\ per machine, due to
non-floating point tasks including bookkeeping and thread synchronization.
Given these considerations, we estimate that a full single-detector Advanced
\LIGO\ search with \lloid\ will require about $\sim$$10$ machines.

We computed the computational cost of a full Advanced \LIGO\ \NS--\NS\ search a
second way by dividing the entire 1--3~$M_\odot$ parameter space into sub-banks
of 657 points apiece, performing time slices and SVDs for each sub-bank, and
tabulating the number of floating point operations using
expression~(\ref{eq:lloid-flops}).  This should be a much more accurate measure
because template length varies over the parameter space.  Lower chirp mass
templates sweep through frequency more slowly and require more computations
while higher chirp mass templates are shorter and require fewer computations.
Despite these subtleties, this estimate gave us $1.1 \times 10^{11}$~\flops,
agreeing with the simple scaling argument above.  Therefore we believe that
$\sim$$10$ machines is a realistic estimate of our computing requirements.

By comparison, using the \TD\ method to achieve the same latency costs
$\sim$$10^{13}$ \flops\ for this particular sub-bank, and so it would require
a total of $\gtrsim$$10^{15}$ \flops\ or $\gtrsim$$10^5$ present-day machines
to search the full parameter space.  Presently, the \LIGO\ Data Grid%
\footnote{\url{https://www.lsc-group.phys.uwm.edu/lscdatagrid/}} consists of
only $\sim$$10^4$ machines, so direct \TD\ convolution is clearly impractical.

\subsection{Measured latency and overhead}

Our \gstreamer\ pipeline for measuring impulse responses contained
instrumentation that would not be necessary for an actual search, including
additional interpolation filters to bring the early-warning outputs back to the
full sample rate and additional outputs for recording signals to disk.

We wrote a second, stripped pipeline to evaluate the actual latency and
computational overhead.  We executed this pipeline on one of the submit
machines of the \LIGO-Caltech cluster, a Sun Microsystems Sun
Fire\texttrademark\ X4600~M2 server with eight quad-core 2.7~GHz AMD
Opteron\texttrademark\ 8384 processors.  This test consumed $\sim$90\% of the
capacity of just one out of the 32 cores, maintaining a constant latency of
$\sim$0.5~s.

The measured overhead is consistent to within an order of magnitude with the
lower bound from the \flops\ budget.  Additional overhead is possibly
dominated by thread synchronization.  A carefully optimized \gstreamer\
pipeline or a hand-tuned C implementation of the pipeline might reduce overhead
further.

The 0.5~s latency is probably due to buffering and synchronization.  The latency
might be reduced by carefully tuning buffer lengths at every stage in the
pipeline.  Even without further refinements, our implementation of the \lloid\
algorithm has achieved latencies comparable to the \LIGO\ data acquisition
system itself.  

\section{Conclusions}

We have demonstrated a computationally feasible filtering algorithm for the rapid
and even early-warning detection of \GW{}s emitted during the coalescence
of neutron stars and stellar-mass black holes.  It is one part of a complicated
analysis and observation strategy that will unfortunately have other sources of
latency.  However, we hope that it will motivate further work to reduce such
technical sources of \GW\ observation latency and encourage the possibility of
even more rapid \EM\ followup observations to catch prompt emission in the
advanced detector era.

\CBC\ events may be the progenitors of some short hard \GRB{}s and are expected
to be accompanied by a broad spectrum of \EM\ signals. Rapid alerts to the
wider astronomical community will improve the chances of detecting an \EM\
counterpart in bands from gamma-rays down to radio. In the Advanced LIGO
era, it appears possible to usefully localize a few rare events prior to the
\GRB{}, allowing multi-wavelength observations of prompt emission. More
frequently, low-latency alerts will be released after merger but will still
yield extended X-ray tails and early afterglows.

The \lloid\ method is as fast as standard \fft\ convolutions but allows for
latency free, real-time operation.  We anticipate requiring $\gtrsim$40
modern multi-core computers to search for binary neutron stars using
coincident \GW\ data from a 4-detector network.  In the future, additional
computational savings could be acheived by conditionally reconstructing the
\SNR\ time-series only during times when a composite detection statistic
crosses a threshold~\citep{svd-compdetstat}.  However, the anticipated required
number of computers is well within the current computing capabilities of the
\LIGO\ Data Grid.

We have shown a prototype implementation of the \lloid\ algorithm using
\gstreamer, an open source signal processing platform.  Work will be done in
order to improve upon the latency of our implementation.  Ultimately the best
possible latency would be achieved by tighter integration between data
acquisition and analysis with dedicated hardware and software. This could be
considered for third-generation detector design.  Also possible for
third-generation instruments, the \lloid\ method could provide the input for a
dynamic tuning of detector response via the signal recycling mirror to match
the frequency of maximum sensitivity to the instantaneous frequency of the
\GW\ waveform.  This is a challenging technique, but it has the potential for
substantial gains in \SNR\ and timing accuracy \citep{PhysRevD.47.2184}.

Although we have demonstrated a computationally feasible statistic
for advance detection, we have not yet explored data calibration and whitening,
triggering, coincidence, and ranking of gravitational wave candidates in a
framework that supports early \EM\ followup.  We will explore these and also
using the time-slice decomposition and the \SVD\ to form low-latency
signal-based vetoes (e.g.~$\chi^2$~statistics) that have been essential for
glitch rejection used in previous \GW\ \CBC\ searches.  These additional stages
may incur some extra overhead, so computing requirements will likely be somewhat
higher than our estimates.

Our future work must more deeply address sky localization accuracy in a
realistic setting as well as observing strategies. Here, we have followed
\citet{Fairhurst2009} in estimating the area of 90\% localization confidence in
terms of timing uncertainties alone, but it would be advantageous to use a
galaxy catalog to inform the telescope tiling \citep{galaxy-catalog}. Because
early detections will arise from nearby sources, the galaxy catalog technique
might be an important ingredient in reducing the fraction of sky that must be
imaged.  Extensive simulation campaigns incorporating realistic binary merger
rates and detector networks will be necessary in order to fully understand the
prospects for early-warning detection, localization, and \EM\ followup using
the techniques we have described.

\acknowledgements

\LIGO\ was constructed by the California Institute of Technology and
Massachusetts Institute of Technology with funding from the National Science
Foundation and operates under cooperative agreement PHY-0107417.  CH would like
to thank Ilya Mandel for many discussions about rate estimates and the prospects
of early detection, Patrick Brady for countless fruitful conversations about low
latency analysis methods, and John Zweizig for disscussions about \LIGO\ data
acquisition.  NF would like to thank Alessandra Corsi and Larry Price for
illuminating discussions on astronomical motivations.  LS would like thank Shaun
Hooper for productive conversations on signal processing.  This research is
supported by the National Science Foundation through a Graduate Research
Fellowship to LS and by the Perimeter Institute for Theoretical Physics through
a fellowship to CH. DK is supported from the Max Planck Gesellschaft. 

This paper has \LIGO\ Document Number {LIGO-P0900004-v27}.

\bibliographystyle{apj}
\bibliography{references}

\end{document}
