\section{Early warning searches for compact binary coalescence}
\label{SECII}\label{sec:method}

In this section we describe a decomposition of the compact binary parameter
space that reduces low-latency filtering cost sufficiently to allow for the
possibility of \earlywarning\ detection with modest computing requirements.  We
expand on the ideas of~\cite{Marion2004, Buskulic2010} that describe a
multiband decomposition of the compact binary parameter space that resulted in
search with $\sim$minutes latency in \LIGO{}'s S6 and Virgo's VSR2 science runs.
We combine this with the orthogonal decomposition described
in~\cite{Cannon:2010p10398} that exploits the redundancy of the template banks.

\subsection{Conventional \CBC{} matched filter searches}

Inspiral signals are parameterized by a set of intrinsic parameters
$\bar{\theta}$ that determine the amplitude and phase evolution of the observed
binary signals. For systems where the effects of spin can be ignored, the
intrinsic parameters are the component masses of the binary, $\bar{\theta} =
(m_1, m_2)$. Searches for gravitational waves from compact binary coalescence
typically employ matched filter banks, called template banks, that discretely
sample the possible intrinsic parameters~\cite{findchirppaper}.  The filters
for the waveforms $x(t,\bar{\theta})$, known as templates, are the waveforms
weighted by the inverse detector noise amplitude spectral density, so as to
whiten them. To construct a template bank, templates are chosen with discrete
signal parameters $\theta_1,\, \theta_2,\, \cdots$, $\theta_N$ to assure a
bounded loss of \SNR~\cite{Owen:1995tm,Owen:1998dk}. That is, any possible
signal within the search space will have a cross-correlation of $\geq0.97$ with
at least one template. Such a template bank is said to have a 97\% {\em
minimum match}. Data are filtered against each template to produce an \SNR\
time-series. Local peak-finding across time and templates determines detection
candidates.
%
\footnote{There are two gravitational-wave polarizations, $+$ and $\times$. A
given detector will observe a combination of these polarizations that will
largely be degenerate with an overall unknown constant phase.  This can be
maximized over by filtering for quadrature phases and taking the magnitude of
the result. For simplicity we will ignore that aspect in this work, as it is
straightforward to generalize but not necessary for understanding any of the
points that will be made.}

We will denote the $i^\mathrm{th}$ filter with parameters $\bar{\theta_i}$
as a function of time $x_i(\tau)$.
In this work we will discuss transformations to a set of filters
$\{x_i(\tau)\}$.  Some of these transformations are not useful or practical
over the entire parameter space.  For that reason we assume from here onward
that the set of filters $\{x_i(\tau)\}$ refer to a set of near-neighbor filters
that can be chosen as a subset of the full parameter space.  Several such
subsets can be chosen until all of the filters are a member of one local set.
\editorial{nvf: I don't think that the splitting of the bank is a crucial point here. Perhaps later if we discuss the up-front cost of the SVD.}

Filtering the detector data $h(t)$ involves a convolution of the data with the
filter.  For a unit-normalized filter, and whitened detector data, the result
can be interpreted as the signal-to-noise ratio, $\rho_i(t)$ and is defined as
%
% Filtering equations
%
\begin{eqnarray}
\rho_i(t) &=& \int x_i(\tau) \, h(t-\tau) \, d \tau \label{eq:SNRTD} \\
          &=& \int \tilde{x}_i(f) \, \tilde{h}(f) \, \e^{-2\pi\i ft} \label{eq:SNRFD} df,
\end{eqnarray}
%
%
where the second line is a result of the convolution theorem and
$\tilde{x}_i(f)$ is the Fourier transform of $x_i(\tau)$ as is $\tilde{h}(f)$ the
Fourier transform of $h(t)$.

The evaluation of the integrals in \eqref{eq:SNRTD} and \eqref{eq:SNRFD} are
implemented as sums over sample points for the digitized gravitational-wave
detector output.  Discrete Fourier transforms can be computed efficiently
numerically.  For that reason \eqref{eq:SNRFD} is typically far faster,
computationally.  To evaluate \eqref{eq:SNRTD} requires \order{\tmpsamps
\hoftsamps} floating point operations per filter $x_i$, where \tmpsamps\ is the
number of sample points in the filter $x_i$ and \hoftsamps\ is the number of
sample points in the data $h$.  Assuming \numtmps\ filters are required, the
total cost is \order{\numtmps \tmpsamps \hoftsamps} However, \eqref{eq:SNRFD}
requires only \order{\hoftsamps \log \hoftsamps} operations per filter assuming
transform lengths that are longer than the filter (i.e. \hoftsamps\ $>$
\tmpsamps), resulting in a total cost of \order{\numtmps \hoftsamps \log
\hoftsamps}.  In most cases \tmpsamps\ $\gg$ $\log$ \hoftsamps\ and the
computational savings by choosing the frequency-domain integral form
\eqref{eq:SNRFD} is clear.  However, to take full advantage of the
computational efficiency of \eqref{eq:SNRFD} requires an acausal knowledge of
the detector data $h(t)$, which implies an inherent latency.  In contrast,
\eqref{eq:SNRTD} can be updated every time a new sample point of detector data
is taken.  

\subsection{Proposed method}

In order to minimize latency we propose using the time-domain convolution
presented in \eqref{eq:SNRTD}.  However, because the brute-force evaluation of
\eqref{eq:SNRTD} is far too costly to be useful, we will consider an
approximation to \eqref{eq:SNRTD} that can reduce substantially the cost of
\realtime\ filtering. This approximation has the form
%
% orthogonal decomposition filtering
%
\begin{equation}
\rho_i(t) \approx \sum_k^{\numslices} \sum_j^{\numsvdtmps} 
	\int_{\tau_{k}}^{\tau_{k+1}} v_{ijk} \sigma_{jk} u_{jk}\,(\tau) \, h(t-\tau) \, d \tau \label{eq:decomp}
\end{equation} 
%
%
where $u_{jk}(\tau)$ is an orthogonal basis set of filters spanning the space
of $\{x_i(\tau)\}$ and $\sigma_{jk} v_{ijk}$ is a tensor relating the filters
$u_{jk}(\tau)$ to the original filter set $\{x_i(\tau)\}$.  We claim that with
a suitable choice of filters $u_{jk}(\tau)$ one can reduce the computational
cost of \eqref{eq:SNRTD} sufficiently to feasibly search for
gravitational waves from compact binary coalescence in \realtime.  This
requires 1) exploiting
the time-frequency characteristics of the binary waveforms and 2) exploiting the redundancy of the template bank. We describe our
procedure for producing the decompostion in \eqref{eq:decomp} in the remainder
of this section.

\subsubsection{Selectively reducing the sample rate of the data and template waveforms}

The first step of the orthogonal decomposition described in \eqref{eq:decomp}
is to divide the templates into \emph{time slices}.  This is a time-domain
analogue to the frequency-domain decomposition described in ~\cite{Marion2004,
Buskulic2010, beauville2006, beauville2008}.  A matched filter is constructed
for each time slice.  The outputs form an ensemble of partial \SNR{}
streams.  By linearity, these partial \SNR\ streams can be suitably time
delayed and summed to reproduce the \SNR\ of the full template.  We will show
in the next section that this, combined with the singular value docomposition,
is sufficient to enable a computationally efficient time-domain search and
furthermore is an essential part of an \earlywarning\ detection scheme.

For concreteness and simplicity, we will consider an inspiral waveform in the
quadrupole approximation, for which the time-frequency relation is
%
\begin{equation} \label{eq:fgw}
%
f = \frac{1}{\mathcal{\pi M}} \left[ \frac{5}{256}\frac{\mathcal{M}}{-t}
\right]^{3/8}.
%
\end{equation}
%
Here, $\mathcal{M}$ is the chirp mass of the binary in units of time (where $G
M_\odot / c^3 \approx 5 \umu\mathrm{s}$) and $t$ is the time relative to the
coalescence of the binary~\cite{findchirppaper, kidder1992}.  Usually the
template is truncated at some prescribed time $t_0$, or equivalently frequency \fmax.
This is often chosen to correspond to the \ISCO\ defined previously. An inspiral signal
will enter the detection band at a low frequency, $f = f_\mathrm{low}$
corresponding to a time $t_\mathrm{low}$.  The template is assumed to be zero
outside the interval $[t_\mathrm{low}, t_\mathrm{0})$ and is said to have  a
duration of $t_\mathrm{0} - t_\mathrm{low}$. It is critically sampled at a
rate of $2 \fmax$.

The monotonic time-frequency relationship of \eqref{eq:fgw} allows us to choose
time-slice boundaries that require substantially less bandwidth at early times
in the inspiral.  Our goal is to reduce the filtering cost of a large fraction
of the waveform by computing part of the filter integral at a lower sample
rate.  Specifically we consider here time slice boundaries with the next
highest power-of-two sample rate that critically samples the time sliced
filter.  The time slices for this template consist of the $k$ intervals $(t_k,
t_{k-1}], \dots, (t_2, t_1], (t_1, t_0]$ sampled at frequencies
$f_\mathrm{k-1}, \dots, f_1, f_0$ where $f_0 \geq 2 f_\mathrm{ISCO}$, $t_0 =
t_\mathrm{ISCO}$, $f_{k-1} \geqslant 2 f_\mathrm{low}$, and $t_k \leqslant
t_\mathrm{low}$. An example time-slice design satisfying these constraints for
a $1.4 - 1.4 \, M_{\odot}$ binary is shown in table~\ref{table:time_slices}.
%
\begin{table}[h!]
\begin{minipage}[c]{0.52\textwidth}
\centering
\vspace{0.8cm}
\includegraphics{time_slices.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering
\input{time_slices.tex}
\end{minipage}
\caption{\label{table:time_slices} Example of nearly critically sampled,
power-of-two time slices for a $1.4 - 1.4 \, M_{\odot}$ template extending from
$f_\mathrm{low} = 10 \, \mathrm{Hz}$ to $f_\mathrm{ISCO} = 1571\, \mathrm{Hz}$
with a time frequency structure given by ($\ref{eq:fgw})$. $f_k$ is the sample
rate of the time slice, $(t_{k+1}, t_k]$ are the boundaries in seconds
preceeding coalescence and \slicessamps\ are the number of sample points in the
$k^{\mathrm{th}}$ filter.}
\end{table}
%

Rather than applying a unique time-slice decomposition to each template
waveform, we find a decomposition that is adequate for the entire set
$\{x_i\}$.  This is facilated by choosing templates in the set $\{x_i\}$ with
similar chirp masses, $\mathcal{M}$.  The time-slice decompositions of the
filters $x_i(\tau)$ lead to new filters $y_{ik}(\tau)$ satisfying
%
\begin{equation}
\label{eq:time_slices}
x_i(\tau) \approx \sum_k^{\numslices} y_{ik}(\tau)
\end{equation}
where \numslices\ is the number of time slices required.  Note that we write
the relationship as approximate since the resampling implementation may have
some inherent loss in quality.  We note that by construction these filters are
orthogonal over the index $k$ since they are disjoint in time.  In the next
section we examine how to reduce the number of filters $y_{ik}(\tau)$ via
singular value decomposition to construct a set of filters that is also
orthogonal within a given time slice.

\subsubsection{Reducing the number of filters with the singular value
decomposition}

As described previously, the template banks are, by design, highly correlated.
It is possible to greatly reduce the number of filters required to achieve a
particular minimum match by designing an appropriate set of orthonormal {\em
basis templates}.  A purely numerical technique based on the application of the
singular value decomposition (\SVD) to inspiral waveforms is demonstrated
in~\cite{Cannon:2010p10398}.  Using the results of ~\cite{Cannon:2010p10398} we
establish that the filters of the previous section can be approximated to high
accuracy by the expansion in the singular value basis
%
\begin{equation}
y_{ik}(\tau) \approx \sum_j^{\numsvdtmps} \sigma_{jk} v_{ijk} u_{j}(\tau)
\label{eq:svddecomp}
\end{equation}
%
where $\sigma_{jk}$ is the $j^{\mathrm{th}}$ singular value for the
$k^{\mathrm{th}}$ time slice, $v_{ijk}$ is an orthogonal matrix for the
$k^{\mathrm{th}}$ time slice  and $u_{jk}$ is a new orthogonal basis filter set
for the $k^{\mathrm{th}}$ time slice.  The authors of \cite{Cannon:2010p10398}
showed that to high reconstruction accuracy far fewer filters are needed than
were in the original template bank. We find that when combined with the
time-slice decomposition, the number of \SVD\ filters, \numsvdtmps\ is much smaller
than the original number of filters \numtmps.  We combine \eqref{eq:svddecomp}
with \eqref{eq:time_slices} to arrive at \eqref{eq:decomp}.  In the next
section we compute the expected computational cost scaling of this
decomposition and compare it with the brute-force implementation in
\eqref{eq:SNRTD} and higher latency \fft\ methods.

\subsection{Comparison of computational costs}

We now examine the computational cost scaling of the approximate implementation
of \eqref{eq:SNRTD} as \eqref{eq:decomp}.  An actual implementation of this
decomposition in a working analysis pipeline is discussed in the next section
along with measured computational requirements.  For convenience, table
\ref{tab:recap} is a recap of the meaning of various symbols used in this
calculation.
%
% symbols used in FLOPs calculations table
%
\begin{table}
\begin{tabular}{rl}
\bf{Symbol}	& \bf{Definition} \\
\hline
\hoftsamps	& The number of sample points in the data \\
\tmpsamps	& The number of sample points in template $x_i$ \\
\numtmps	& The number of templates in the set $\{x_i\}$ \\
\numslices	& The number of time slices \\
\numsvdtmps	& The number of orthogonal filters in time slice $k$ \\
\slicessamps	& The number of samples in time slice $k$ \\
\fmax		& The maximum frequency of a filter \\
$f_k$		& The sample frequency of the $k^{\mathrm{th}}$ filter \\
\resampsamps	& The number of sample points in the resample filter
\end{tabular}
\caption{\label{tab:recap} Notation used to describe filtering.  This table
provides a quick reference for symbols used.}
\end{table}

In table \ref{table:flops} we present the computational cost scaling in
floating point operations per sample for common tasks in the pipeline.  
%
% common tasks FLOPS table
%
\begin{table}[htdp]
\begin{center}
\begin{tabular}{l l}
\bf{Process} & \bf{ops/sample} \\
\hline
% FIR
\fir\ matched filter, \numtmps\ templates of length \tmpsamps\ & $2 \numtmps \tmpsamps$ \\
% FFT
\fft\ matched filter, \numtmps\ templates of length \tmpsamps\, & \multirow{2}{*}{$\mbox{\fontsize{14}{17}\selectfont $\frac{4 (\numtmps + 1) \lg \fftblock + 2 \numtmps}{1 - \tmpsamps/\fftblock}$}$} \\
% FFT contd...
$\,\,\,\,\,\,$ blocks of length \fftblock & \\
% Resampling
\fir\ resampling filter, length \resampsamps\ for each of \numtmps\ templates& \multirow{2}{*}{$2 \numtmps \resampsamps\ f_1 / f_2$} \\
% Resampling contd...
$\,\,\,\,\,\,$ and sample rates $f_1 < f_2$ & \\
% Matrix multiply
multiply $M \times L$ real matrix by $L\times1$ real vector & $2 M L$ \\
%
\end{tabular}
\end{center}
\caption{Number of floating point operations per sample (multiplications and
divisions) required for a selection of signal processing operations used in
\textsc{lloid}.}
\label{table:flops}
\end{table}

The filter bank can be implemented using finite impulse response (\fir{}) filters,
which are just sliding window dot products.  If there are \numtmps\ templates
of length \tmpsamps\, and the data stream contains \hoftsamps\ samples, then
applying the filter bank requires $2 \numtmps \tmpsamps \hoftsamps$ operations.

More commonly, the matched filters are implemented using the \fft\ convolution.
This entails applying {\fft}s to blocks of \fftblock\ samples, with $\tmpsamps
\leq \fftblock$, each block overlapping the previous one by $ \fftblock -
\tmpsamps $ samples.  There are $\hoftsamps/(\fftblock-\tmpsamps)$ such blocks
required to filter \hoftsamps\ samples of data.  Modern implementations of the
Cooley-Tukey \fft, such as the ubiquitous \texttt{fftw}, require about $4 N \lg
N$ operations to evaluate a \textsc{dft} of size $N$~\cite{Johnson:2007p9654}.
%
%
\editorial{This is more commonly known as ``overlap-save''.  We should find
someone else's operation count and cite it.}  
%
%
A \fftblock\ sample cross-correlation consists of a forward \fft, a \fftblock
sample dot product, and an inverse \fft\, totaling $8 \fftblock \lg \fftblock +
2 D$ operations per block.  Per sample, this is $(8 \lg \fftblock + 2) / (1 -
\tmpsamps/\fftblock)$ operations.  As this expression indicates the number
of operations increases as the block size \fftblock\ approaches the
filter length \tmpsamps.
%
%
\editorial{Drew: Why don't we change this to an overlap of $m$ samples so we
can see what happens as we increase the overlap to reduce latency.}

The \fir\ filter implementation has the advantage that it has no intrinsic
latency, whereas the \fft\ convolution has latency of $\fftblock-\tmpsamps$. 
%
%
%For example, for a $1.4 -
%1.4 \, M_\odot$ template with duration $\sim 1 \, \mathrm{ks}$, the
%\textsc{fft} convolution has a latency $\geq 2 \, \mathrm{ks}$.  
%
However, the \fir\ filter implementation has the disadvantage of much greater
overhead per sample than the \fft\ convolution.  For a $1\,\mathrm{ks}$
template sampled at $4096\,\mathrm{Hz}$, the \textsc{fir} implementation
requires about about $\tmpsamps / 8 \lg 2 \tmpsamps = 2.2 \times 10^4$ times
more operations per sample than the \fft implementation.  We will now consider
the computational cost of the \fir\ filter implementation described in the
previous sections.

It is convenient to express the computational cost of the entire filtering
procedure in floating point operations per second {\flops}.  The cost will be
the sum of the cost of the \fir\ filtering for the orthogonal filter in each
time slice plus the cost of reconstructing the original waveforms with matrix
operations and resampling.  Using the formulas in table \ref{table:flops} we
arrive at
%
\begin{equation}
%\mathrm{Computational\, cost} 
\mathrm{C.C.} \propto 2 \sum_k^{\numslices} (\slicessamps + \numtmps) \numsvdtmps f_k \nonumber
	r \sum_{k, f_k \neq f_{\fmax}}^{\numslices} \numtmps \resampsamps f_k 
\label{eq:TDcost}
\end{equation}
%
where the first sum is the total cost for filtering and reconstructing the
orthogonal filters $u_{jk}$.  The second sum is the cost of resampling the
reconstructed time-slice outputs to the original sample rate {\fmax}.  It
assumes a \fir\ filter with \resampsamps\ sample points.  Note that the cost
for resampling only occurs for the downsampled time slices. The resampling cost
largely depends on {\resampsamps}.  The computational cost of \eqref{eq:TDcost}
is dominated by the highest frequency terms in the sum.  Comparing just the
highest frequency term of \eqref{eq:TDcost} with \eqref{eq:SNRTD} shows \order{
(\slicesamps{\mathrm{max}} + \numtmps) \svdtmps{\mathrm{max}} \fmax} \flops\
versus \order{\numtmps \tmpsamps \fmax} {\flops}.  The full calculation for a
particular patch of parameter space is shown in table \ref{table:comp_cost}.
\hl{FIXME we need to actually say what went into this} 


\editorial{From here down introduce an example, but don't go through the FFT
methods in lloid focus just on the comparison of the TD method without lloid
tricks, the FD method without lloid tricks, and the TD lloid method (no comp
detection statistic though), remember that the goal is near realtime detection
not computational cost improvements on FD methods}


\editorial{It would be good to illustrate the layout of this particular
template bank: masses spanned, time-slice layout \dots}
\begin{table}[!h]
\label{table:comp_cost}
\begin{center}
\input{flop_budget_example}
\end{center}
\caption{\hl{FIXME: Operation counts per sample for four different detection methods.}}
\end{table}


\begin{comment}

% Do we want to spend this much copy on the whitener?
% What matters is that we have picked a low-latency whitening procedure.

\subsection{Data Whitening}
Matched filtering the SVD basis vectors has motivated a decoupling of the
whitening routine from the matched filtering engine. This was necessary in
order weight templates appropriately by whitening them before the SVD
calculation, and, as such, we have also moved the data whitening outside the
match filter engine. We have chosen to whiten the data using a running
geometric average of the PSD computed by Hann windowing 8 second buffers of
data and using 50\% overlapping buffers. The running geometric average is
updated once per buffer from a median of the recent PSDs. We find that this
algorithm is extremely fast to converge to an accurate PSD and also very robust
against glitches.

An inspiral, to leading order, has gravitational-wave frequency evolution given
by
%
\begin{equation}
%
f(t) = \frac{1}{8 \pi \mathcal{M}} \left( \frac{t_c - t}{5 \mathcal{M}}
\right)^{-3/8}\;,
%
\end{equation}
%
whose time derivative is given by
%
\begin{equation}
%
\frac{df}{dt} = \frac{3}{320 \pi \mathcal{M}^2} \left( \frac{t_c - t}{5
\mathcal{M}} \right)^{-11/8}\;.
%
\end{equation}
%
Combining these, we find the frequency evolution as a function of frequency to
be
%
\begin{equation}
%
\frac{df}{dt}(f_0) = \frac{3}{320 \pi \mathcal{M}^2} \left( 8 \pi \mathcal{M}
f_0 \right)^{3/11}\;,
%
\end{equation}
%
which can be inverted to get the minimum frequency which has a given frequency
derivative,
%
\begin{equation}
%
f_0 = \frac{1}{8 \pi \mathcal{M}} \left( \frac{320 \pi \mathcal{M}^2}{3}
\frac{df}{dt} \right)^{11/3}\;.
%
\end{equation}

If we allow frequency bins to be affected by an injection only once within the
median history, we need to calculate the corresponding $df/dt$ for our PSD
calculation. An FFT of a buffer with length $T$ will have a frequency
resolution $df = 1/(2T)$. Hann windowing the data and overlapping buffers by
50\% introduces correlations in neighboring frequency bins of the FFT. This
means that we actually want the frequency to change by $df = 3/(2T)$ before the
next PSD calculation, which happens a $dt = T/2$ later, resulting in a minimum
$df/dt = 3/T^2$.

Combining these results we find the lower frequency bound for our injections,
assuming a chirp mass for a binary with $m_1 = m_2 = 1 M_{\odot}$ and $T = 8$
s, is 23 Hz.

We want to compute the equivalent injection density which would be biased as
much as lalapps\_inspiral's PSD estimator, which allows $\sim3$ injections per
2048 seconds. It computes the PSD by breaking the segment up into 16 256 second
chunks. These chunks are then combined into to sets of 8 chunks each. The
median of each set is then averaged across the sets for each frequency bin to
produce the PSD estimate for that 2048 second segment. This procedure results
in an average injection density of 1.5 per 8 PSDs in the median history, which
is comparable to LLOID's 1 per 7 PSDs in the median history. Since LLOID has an
injection history of 7 PSDs which span 32 seconds, this means LLOID can perform
one injection every 32 seconds, or 64 injections per 2048 second segment, more
than an order of magnitude increase in density over lalapps\_inspiral.
\end{comment}
