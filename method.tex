\section{Early warning searches for compact binary coalescence}
\label{SECII}\label{sec:method}

In this section we describe a decomposition of the compact binary parameter
space that reduces low-latency filtering cost sufficiently to allow for the
possibility of \earlywarning\ detection with modest computing requirements.  We
expand on the ideas of~\cite{Marion2004, Buskulic2010} that describe a
multiband decomposition of the compact binary parameter space that resulted in a
search with minutes latency in \LIGO{}'s S6 and Virgo's VSR2 science runs.
We combine this with the orthogonal decomposition described
in~\cite{Cannon:2010p10398} that exploits the redundancy of the template banks.

\subsection{Conventional \CBC{} matched filter searches}

Inspiral signals are continuously parameterized by a set of intrinsic source parameters
$\vec\theta$ that determine the amplitude and phase evolution of the gravitational wave strain. For systems where the effects of spin can be ignored, the
intrinsic source parameters are the component masses of the binary, $\vec\theta =
(m_1, m_2)$. Searches for inspiral signals
typically employ matched filter banks that discretely
sample the possible intrinsic parameters~\cite{findchirppaper}.  For a given source, the strain observed by the detector is a linear combination of two waveforms corresponding to the `$+$' and `$\times$' gravitational wave polarizations, so for any value of $\vec\theta$ we must implement 2 filters.  The coefficients for the $M$ filters
are known as templates, and are formed by discretizing and time reversing the waveforms and weighting them by the inverse amplitude spectral density of the detector's noise. To construct a template bank, templates are chosen with the $M/2$ discrete signal parameters $\vec\theta_0,\, \vec\theta_1,\, \dots,\, \vec\theta_{M/2-1}$ to assure a
bounded loss of \SNR~\cite{Owen:1995tm,Owen:1998dk}. That is, any possible
signal within the a given range of intrinsic source parameters will have an inner product that is $\geqslant 0.97$ with
at least one template. Such a template bank is said to have a {\em
minimum match} of 0.97. The data from the detector are whitened and convolved with each template to produce $2M$ \SNR\
time series. Local peak-finding across time and templates determines detection
candidates.

Filtering the detector data involves a convolution of the data with the
template.  For a unit-normalized template $h_i[k]$ and whitened detector data $x[k]$, both sampled at a rate $f^0$, the result can be interpreted as the signal-to-noise ratio, $\rho_i[k]$ defined as
%
% Filtering equations
%
\begin{equation}
	\label{eq:SNRTD}
	\rho_i [k] = \sum_{n=0}^{N-1} h_{i}[n] x [k-n].
\end{equation}

Equation~(\ref{eq:SNRTD}) can be implemented in the time domain as an \textsc{fir} filter, requiring $\mathcal O(\numtmps \tmpsamps)$ floating point operations per sample.  However, it is typically much more computationally efficient to use the convolution theorem and the \textsc{fft} to implement fast convolution, requiring only $\mathcal O(\numtmps \lg \tmpsamps)$ operations per sample.


\subsection{The \lloid\ method}

In the remainder of this section we explore a method for reducing the
computational cost of a time domain search for compact binary coalescence.  We
will show that it is possible to have a theoretically zero latency algorithm
that competes with the computational cost of the standard frequency domain
matched filtering, which requires a signficant latency in order to be
computationally cheap. Our method, \lloid\ involves two transformations to the
template waveforms that produce a set of orthogonal filters with far fewer
sample points than the original template waveforms.  The reduction in the
number of sample points in all of the filters required to search the entire
parameter space is dramatic.  

The first transformation is to chop the time domain templates up into time
slices.  Since each template piece is disjoint in time, the resulting set for a
single template is orthogonal.  Given the chirp like structure of the
templates, the early time slices have significantly lower bandwidth and can be
safely downsampled.  The downsampling removes a factor of $\sim 10$ samples on
average and also allows the filters to be evaluated at about a $\sim 10$ times
lower rate on average.  This amounts to more than a factor of 100 in the
floating point operations per second required.  However, the resulting filters
are still not orthogonal across the mass parameter space, and are in fact
highly redundant.  We use the singular value decomposition to produce an
orthogonal filter set from the time sliced templates~\cite{Cannon:2010p10398}.
We find that this reduces the number of sample points that need to be filtered
by another factor of $\sim 100$.  The combined methods reduce the number of
floating point operations to the level where they are competitive with the
conventional matched filter approach.  In the remainder of this section we
describe the \lloid\ algorithm in detail and provide some basic computational
cost scaling.  

\subsection{Selectively reducing the sample rate of the data and template waveforms}

The first step of our proposed method is to divide the templates into
\emph{time slices} in a time-domain analogue to the frequency-domain
decomposition described in ~\cite{Marion2004, Buskulic2010, beauville2006,
beauville2008}\editorial{Four citations is way too many!  We should pick just
\emph{one} citation for \textsc{mbta}, and it should be the most recent and
complete description of it.}.  A matched filter is constructed for each time
slice.  The outputs form an ensemble of partial \SNR{} streams.  By linearity,
these partial \SNR\ streams can be suitably time delayed and summed to
reproduce the \SNR\ of the full template.  To wit,
%
\begin{equation}
\label{eq:time-slices}
h_{i}[k] = \sum_{s=0}^{S-1}
	\cases{%
		h_i^s[k] & for $t^s \leqslant k / f^0 < t^{s+1}$ \\
		0 & otherwise
	}
\end{equation}
%
for $S$ integers $\{f^0 t^s\}$ such that $0  = f^0 t^0 < f^0 t^1 < \cdots < f^0 t^S = N$.
We will show
in the next section that this, combined with the singular value docomposition,
is sufficient to enable a computationally efficient time-domain search and
furthermore is an essential part of an \earlywarning\ detection scheme.

For concreteness and simplicity, we will consider an inspiral waveform in the
quadrupole approximation, for which the time-frequency relation is
%
\begin{equation} \label{eq:fgw}
%
f = \frac{1}{\mathcal{\pi M}} \left[ \frac{5}{256}\frac{\mathcal{M}}{-t}
\right]^{3/8}.
%
\end{equation}
%
Here, $\mathcal{M}$ is the chirp mass of the binary in units of time (where $G
M_\odot / c^3 \approx 5 \umu\mathrm{s}$) and $t$ is the time relative to the
coalescence of the binary~\cite{findchirppaper, kidder1992}.  Usually the
template is truncated at some prescribed time $t^0$, or equivalently frequency $f_\textrm{hi}$.
This is often chosen to correspond to the \ISCO. An inspiral signal
will enter the detection band at a low frequency, $f = f_\mathrm{low}$,
corresponding to a time $t_\mathrm{low}$.  The template is assumed to be zero
outside the interval $[t_\mathrm{low}, t^0)$ and is said to have  a
duration of $t^0 - t_\mathrm{low}$. It is critically sampled at a
rate of $2 f_\mathrm{hi}$.

The monotonic time-frequency relationship of \eqref{eq:fgw} allows us to choose
time-slice boundaries that require substantially less bandwidth at early times
in the inspiral.  Our goal is to reduce the filtering cost of a large fraction
of the waveform by computing part of the filter integral at a lower sample
rate.  Specifically we consider here time slice boundaries with the next
highest power-of-two sample rate that critically samples the time sliced
template.  The time slices consist of the $S$ intervals $\left(t^S,
t^{S-1}\right],\, \dots,\, \left(t^2, t^1\right],\, \left(t^1, t^0\right]$ sampled at frequencies
$f^\mathrm{S-1},\, \dots,\, f^1,\, f^0$ where $f^0 \geqslant 2 f_\mathrm{hi}$ and $f^{S-1} \geqslant 2 f_\mathrm{low}$.  The time sliced templates may be downsampled without aliasing, so we define them as
%
\begin{equation}
\label{eq:time-sliced-templates}
h_{i}^{s}[k] \equiv
	\cases{
		h_{i}\!\left[k\frac{f}{f^s}\right] & if $t^s \leqslant k/f^s < t^{s+1}$ \\
		0 & otherwise.
	}
\end{equation}
%
An example time-slice design satisfying these constraints for a $1.4 - 1.4 \, M_{\odot}$ binary is shown in table~\ref{table:time_slices}.
%
\begin{table}[h!]
\begin{minipage}[c]{0.52\textwidth}
\centering
\vspace{0.8cm}
\includegraphics{time_slices.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering
\input{time_slices.tex}
\end{minipage}
\caption{\label{table:time_slices} Example of nearly critically sampled,
power-of-two time slices for a $1.4 - 1.4 \, M_{\odot}$ template extending from
$f_\mathrm{low} = 10 \, \mathrm{Hz}$ to $f_\mathrm{ISCO} = 1571\, \mathrm{Hz}$
with a time frequency structure given by ($\ref{eq:fgw})$. $f^s$ is the sample
rate of the time slice, $(t^{s+1}, t^s]$ are the boundaries in seconds
preceeding coalescence and \slicessamps\ are the number of sample points in the
$s^{\mathrm{th}}$ filter.}
\end{table}

Since waveforms with neighboring intrinsic source parameters $\vec\theta$ have similar
time-frequency evolution, it is possible to design computationally efficient time slices
for an extended region of parameter space rather than to design different time slices for
each template.

We note that
the time slice decomposition in equation~(\ref{eq:time-slices}) is manifestly
orthogonal since the time slices are disjoint in time.  In the next
section we examine how to reduce the number of filters within each time slice via
singular value decomposition of the time sliced templates.

\subsection{Reducing the number of filters with the singular value
decomposition}

As described previously, the template banks are, by design, highly correlated.
It is possible to greatly reduce the number of filters required to achieve a
particular minimum match by designing an appropriate set of orthonormal {\em
basis templates}.  A purely numerical technique based on the application of the
singular value decomposition (\SVD) to inspiral waveforms is demonstrated
in~\cite{Cannon:2010p10398}.  Using the results of ~\cite{Cannon:2010p10398} we
establish that the filters of the previous section can be approximated to high
accuracy by the expansion in the singular value basis
%
\begin{equation}
y_{ik}(\tau) \approx \sum_j^{\numsvdtmps} \sigma_{jk} v_{ijk} u_{j}(\tau)
\label{eq:svddecomp}
\end{equation}
%
where $\sigma_{jk}$ is the $j^{\mathrm{th}}$ singular value for the
$k^{\mathrm{th}}$ time slice, $v_{ijk}$ is an orthogonal matrix for the
$k^{\mathrm{th}}$ time slice  and $u_{jk}$ is a new orthogonal basis filter set
for the $k^{\mathrm{th}}$ time slice.  The authors of \cite{Cannon:2010p10398}
showed that to high reconstruction accuracy far fewer filters are needed than
were in the original template bank. We find that when combined with the
time-slice decomposition, the number of \SVD\ filters, \numsvdtmps\ is much smaller
than the original number of filters \numtmps.  We combine \eqref{eq:svddecomp}
with \eqref{eq:time-slices} to arrive at \eqref{eq:decomp}.  In the next
section we compute the expected computational cost scaling of this
decomposition and compare it with the brute-force implementation in
\eqref{eq:SNRTD} and higher latency \fft\ methods.

\subsection{Proposed method}

\editorial{I just dropped our new notation in here; I haven't edited the text yet.}%
Definition of decimated detector data:
\begin{equation}
	x^{s+1}[k] = \left( H^\shortdownarrow x^s\right)[k]
\end{equation}

In order to minimize latency we propose using the time-domain convolution
presented in \eqref{eq:SNRTD}.  However, because the brute-force evaluation of
\eqref{eq:SNRTD} is far too costly to be useful, we will consider an
approximation to \eqref{eq:SNRTD} that can reduce substantially the cost of
\realtime\ filtering. This approximation has the form
%
% orthogonal decomposition filtering
%
\begin{equation}
	\rho_i^s [k] =%
		% Reconstruction
		\underbrace{
			\sum_{\mathclap{l=0}}^{\mathclap{L^s-1}} v_{il}^s \sigma_l^s
		}_\textrm{\clap{reconstruction}}
		% Orthogonal FIR filter
		\overbrace{
			\sum_{\mathclap{n=0}}^{\mathclap{N^s-1}} u_l^s[n] x^s[k-n]
		}^\textrm{\clap{orthogonal {\sc fir} filters}}
		% Plus ...
		+
		% Interpolation SNR
		\underbrace{
			\left(H^\uparrow \rho_i^{s+1}\right)[k]
		}_\textrm{\clap{{\sc snr} from previous time slices}}
\end{equation}
%
%
where $u_{jk}(\tau)$ is an orthogonal basis set of filters spanning the space
of $\{x_i(\tau)\}$ and $\sigma_{jk} v_{ijk}$ is a tensor relating the filters
$u_{jk}(\tau)$ to the original filter set $\{x_i(\tau)\}$.  We claim that with
a suitable choice of filters $u_{jk}(\tau)$ one can reduce the computational
cost of \eqref{eq:SNRTD} sufficiently to feasibly search for
gravitational waves from compact binary coalescence in \realtime.  This
requires 1) exploiting
the time-frequency characteristics of the binary waveforms and 2) exploiting the redundancy of the template bank. We describe our
procedure for producing the decompostion in \eqref{eq:decomp} in the remainder
of this section.

\subsection{Comparison of computational costs}

We now examine the computational cost scaling of the approximate implementation
of \eqref{eq:SNRTD} as \eqref{eq:decomp}.  An actual implementation of this
decomposition in a working analysis pipeline is discussed in the next section
along with measured computational requirements.  For convenience, table
\ref{tab:recap} is a recap of the meaning of various symbols used in this
calculation.
%
% symbols used in FLOPs calculations table
%
\begin{table}
\begin{tabular}{rl}
\bf{Symbol}	& \bf{Definition} \\
\hline
\tmpsamps	& The number of sample points per template \\
\numtmps		& The number of templates \\
\numslices	& The number of time slices \\
\numsvdtmps	& The number of orthogonal templates in time slice $s$ \\
\slicessamps	& The number of samples in time slice $s$ \\
$f^s$		& The sample rate in time slice $s$ \\
\resampsamps	& The number of coefficients in the decimation and interpolation filters
\end{tabular}
\caption{\label{tab:recap} Notation used to describe filters.  This table
provides a quick reference for symbols used.}
\end{table}

In table \ref{table:flops} we present the computational cost scaling in
floating point operations per sample for common tasks in the pipeline.  
%
% common tasks FLOPS table
%
\begin{table}[htdp]
\begin{center}
\begin{tabular}{l l}
\bf{Process} & \bf{ops/sample} \\
\hline
% FIR
\fir\ matched filter, \numtmps\ templates of length \tmpsamps\ & $2 \numtmps \tmpsamps$ \\
% FFT
\fft\ matched filter, \numtmps\ templates of length \tmpsamps\, & \multirow{2}{*}{$\mbox{\fontsize{14}{17}\selectfont $\frac{4 (\numtmps + 1) \lg \fftblock + 2 \numtmps}{1 - \tmpsamps/\fftblock}$}$} \\
% FFT contd...
$\,\,\,\,\,\,$ blocks of length \fftblock & \\
% Resampling
\fir\ resampling filter, length \resampsamps\ for each of \numtmps\ templates& \multirow{2}{*}{$2 \numtmps \resampsamps\ f_1 / f_2$} \\
% Resampling contd...
$\,\,\,\,\,\,$ and sample rates $f_1 < f_2$ & \\
% Matrix multiply
multiply $M \times L$ real matrix by $L\times1$ real vector & $2 M L$ \\
%
\end{tabular}
\end{center}
\caption{Number of floating point operations per sample (multiplications and
divisions) required for a selection of signal processing operations used in
\textsc{lloid}.}
\label{table:flops}
\end{table}

\begin{comment}
The filter bank can be implemented using finite impulse response (\fir{}) filters,
which are just sliding window dot products.  If there are \numtmps\ templates
of length \tmpsamps\, and the data stream contains \hoftsamps\ samples, then
applying the filter bank requires $2 \numtmps \tmpsamps \hoftsamps$ operations.

More commonly, the matched filters are implemented using the \fft\ convolution.
This entails applying {\fft}s to blocks of \fftblock\ samples, with $\tmpsamps
\leq \fftblock$, each block overlapping the previous one by $ \fftblock -
\tmpsamps $ samples.  There are $\hoftsamps/(\fftblock-\tmpsamps)$ such blocks
required to filter \hoftsamps\ samples of data.  Modern implementations of the
Cooley-Tukey \fft, such as the ubiquitous \texttt{fftw}, require about $4 N \lg
N$ operations to evaluate a \textsc{dft} of size $N$~\cite{Johnson:2007p9654}.
%
%
\editorial{This is more commonly known as ``overlap-save''.  We should find
someone else's operation count and cite it.}  
%
%
A \fftblock\ sample cross-correlation consists of a forward \fft, a \fftblock
sample dot product, and an inverse \fft\, totaling $8 \fftblock \lg \fftblock +
2 D$ operations per block.  Per sample, this is $(8 \lg \fftblock + 2) / (1 -
\tmpsamps/\fftblock)$ operations.  As this expression indicates the number
of operations increases as the block size \fftblock\ approaches the
filter length \tmpsamps.
%
%
\editorial{Drew: Why don't we change this to an overlap of $m$ samples so we
can see what happens as we increase the overlap to reduce latency.}

The \fir\ filter implementation has the advantage that it has no intrinsic
latency, whereas the \fft\ convolution has latency of $\fftblock-\tmpsamps$. 
%
%
%For example, for a $1.4 -
%1.4 \, M_\odot$ template with duration $\sim 1 \, \mathrm{ks}$, the
%\textsc{fft} convolution has a latency $\geq 2 \, \mathrm{ks}$.  
%
However, the \fir\ filter implementation has the disadvantage of much greater
overhead per sample than the \fft\ convolution.  For a $1\,\mathrm{ks}$
template sampled at $4096\,\mathrm{Hz}$, the \textsc{fir} implementation
requires about about $\tmpsamps / 8 \lg 2 \tmpsamps = 2.2 \times 10^4$ times
more operations per sample than the \fft{} implementation.  We will now consider
the computational cost of the \fir\ filter implementation described in the
previous sections.

It is convenient to express the computational cost of the entire filtering
procedure in floating point operations per second {\flops}.  The cost will be
the sum of the cost of the \fir\ filtering for the orthogonal filter in each
time slice plus the cost of reconstructing the original waveforms with matrix
operations and resampling.  Using the formulas in table \ref{table:flops} we
arrive at
%
\begin{equation}
%\mathrm{Computational\, cost} 
\mathrm{C.C.} \propto 2 \sum_k^{\numslices} (\slicessamps + \numtmps) \numsvdtmps f_k \nonumber
	r \sum_{k, f_k \neq f_{\fmax}}^{\numslices} \numtmps \resampsamps f_k 
\label{eq:TDcost}
\end{equation}
%
where the first sum is the total cost for filtering and reconstructing the
orthogonal filters $u_{jk}$.  The second sum is the cost of resampling the
reconstructed time-slice outputs to the original sample rate {\fmax}.  It
assumes a \fir\ filter with \resampsamps\ sample points.  Note that the cost
for resampling only occurs for the downsampled time slices. The resampling cost
largely depends on {\resampsamps}.  The computational cost of \eqref{eq:TDcost}
is dominated by the highest frequency terms in the sum.  Comparing just the
highest frequency term of \eqref{eq:TDcost} with \eqref{eq:SNRTD} shows \order{
(\slicesamps{\mathrm{max}} + \numtmps) \svdtmps{\mathrm{max}} \fmax} \flops{}
versus \order{\numtmps \tmpsamps \fmax} {\flops}.  The full calculation for a
particular patch of parameter space is shown in table \ref{table:comp_cost}.
\hl{FIXME we need to actually say what went into this} 


\editorial{From here down introduce an example, but don't go through the FFT
methods in lloid focus just on the comparison of the TD method without lloid
tricks, the FD method without lloid tricks, and the TD lloid method (no comp
detection statistic though), remember that the goal is near realtime detection
not computational cost improvements on FD methods}

\end{comment}


\begin{comment}

% Do we want to spend this much copy on the whitener?
% What matters is that we have picked a low-latency whitening procedure.

\subsection{Data Whitening}
Matched filtering the SVD basis vectors has motivated a decoupling of the
whitening routine from the matched filtering engine. This was necessary in
order weight templates appropriately by whitening them before the SVD
calculation, and, as such, we have also moved the data whitening outside the
match filter engine. We have chosen to whiten the data using a running
geometric average of the PSD computed by Hann windowing 8 second buffers of
data and using 50\% overlapping buffers. The running geometric average is
updated once per buffer from a median of the recent PSDs. We find that this
algorithm is extremely fast to converge to an accurate PSD and also very robust
against glitches.

An inspiral, to leading order, has gravitational-wave frequency evolution given
by
%
\begin{equation}
%
f(t) = \frac{1}{8 \pi \mathcal{M}} \left( \frac{t_c - t}{5 \mathcal{M}}
\right)^{-3/8}\;,
%
\end{equation}
%
whose time derivative is given by
%
\begin{equation}
%
\frac{df}{dt} = \frac{3}{320 \pi \mathcal{M}^2} \left( \frac{t_c - t}{5
\mathcal{M}} \right)^{-11/8}\;.
%
\end{equation}
%
Combining these, we find the frequency evolution as a function of frequency to
be
%
\begin{equation}
%
\frac{df}{dt}(f_0) = \frac{3}{320 \pi \mathcal{M}^2} \left( 8 \pi \mathcal{M}
f_0 \right)^{3/11}\;,
%
\end{equation}
%
which can be inverted to get the minimum frequency which has a given frequency
derivative,
%
\begin{equation}
%
f_0 = \frac{1}{8 \pi \mathcal{M}} \left( \frac{320 \pi \mathcal{M}^2}{3}
\frac{df}{dt} \right)^{11/3}\;.
%
\end{equation}

If we allow frequency bins to be affected by an injection only once within the
median history, we need to calculate the corresponding $df/dt$ for our PSD
calculation. An FFT of a buffer with length $T$ will have a frequency
resolution $df = 1/(2T)$. Hann windowing the data and overlapping buffers by
50\% introduces correlations in neighboring frequency bins of the FFT. This
means that we actually want the frequency to change by $df = 3/(2T)$ before the
next PSD calculation, which happens a $dt = T/2$ later, resulting in a minimum
$df/dt = 3/T^2$.

Combining these results we find the lower frequency bound for our injections,
assuming a chirp mass for a binary with $m_1 = m_2 = 1 M_{\odot}$ and $T = 8$
s, is 23 Hz.

We want to compute the equivalent injection density which would be biased as
much as lalapps\_inspiral's PSD estimator, which allows $\sim3$ injections per
2048 seconds. It computes the PSD by breaking the segment up into 16 256 second
chunks. These chunks are then combined into to sets of 8 chunks each. The
median of each set is then averaged across the sets for each frequency bin to
produce the PSD estimate for that 2048 second segment. This procedure results
in an average injection density of 1.5 per 8 PSDs in the median history, which
is comparable to LLOID's 1 per 7 PSDs in the median history. Since LLOID has an
injection history of 7 PSDs which span 32 seconds, this means LLOID can perform
one injection every 32 seconds, or 64 injections per 2048 second segment, more
than an order of magnitude increase in density over lalapps\_inspiral.
\end{comment}
