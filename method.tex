\section{Novel near real-time algorithm for \CBC\ detection}
\label{sec:method}

In this section we describe a decomposition of the \CBC{} signal space that
reduces \TD\ filtering cost sufficiently to allow for the
possibility of \earlywarning\ detection with modest computing requirements.  We
expand on the ideas of \citet{Marion2004} and \citet{Buskulic2010} that describe a
multi-band decomposition of the compact binary signal space that resulted in
a search with minutes latency during S6/VSR3~\citep{HugheyGWPAW2011}.  We combine this
with the \SVD\ rank-reduction method of \citet{Cannon:2010p10398} that exploits
the redundancy of the template banks.

\subsection{Conventional \CBC{} searches}

Inspiral signals are continuously parameterized by a set of intrinsic source
parameters $\mathbf\Theta$ that determine the amplitude and phase evolution of the
\GW{} strain. For systems in which the effects of spin can be ignored, the intrinsic
source parameters are just the component masses of the binary,
 $\mathbf\Theta = (m_1, m_2)$. For a given source, the strain observed by the
 detector is a linear combination of two waveforms corresponding to the
`$+$' and `$\times$' \GW{} polarizations.  Thus, we must design two filters
for each $\mathbf\Theta$.

Searches for inspiral signals typically employ matched filter
banks that discretely sample the possible intrinsic parameters~\citep{findchirppaper}.
The coefficients for the $\numtmps$ filters are known as templates, 
and are formed by discretizing and time reversing the
waveforms and weighting them by the inverse amplitude spectral density of the
detector's noise.
To construct a template bank, templates are chosen with
$\numtmps/2$ discrete signal parameters $\mathbf\Theta_0,\, \mathbf\Theta_1,\, \dots,\,
\mathbf\Theta_{\numtmps/2-1}$. These are chosen such that any possible signal
will have an inner product $\geqslant$0.97 with at least one template.
Such a template bank is said to have a {\em minimal match} of 0.97~\citep{Owen:1998dk}.

Filtering the detector data involves a convolution of the data with the
templates.  For a unit-normalized template $h_i[k]$ and whitened detector data
$x[k]$, both sampled at a rate $f^0$, the result can be interpreted as the
\SNR{}, $\rho_i[k]$ defined as
%
% Filtering equations
%
\begin{equation}
	\label{eq:SNRTD}
	\rho_i [k] = \sum_{n=0}^{N-1} h_{i}[n] x [k-n].
\end{equation}
This results in $\numtmps$ \SNR\ time series. Local peak-finding across time and
template indices results in single-detector triggers.  Coincidences are sought
between triggers in different \GW\ detectors in order to form detection candidates.

Equation~(\ref{eq:SNRTD}) can be implemented in the time-domain as bank of
finite impulse response (\fir) filters, requiring $\mathcal O(\numtmps
\tmpsamps)$ floating point operations per sample.  However, it is typically
much more computationally efficient to use the convolution theorem and the
\fft\ to implement fast convolution in the frequency domain, requiring only
$\mathcal O(\numtmps \lg \tmpsamps)$ operations per sample but incurring
a latency of $\mathcal O(\tmpsamps)$ samples.


\subsection{The \lloid\ method}

Here we describe a method for reducing the computational cost of a \TD\ search
for compact binary coalescence.  We give a nearly zero-latency algorithm
that competes in terms of floating point operations per second with the
conventional \FD\ method, which by contrast requires a significant latency due to the
inherent acausality of the Fourier transform.  Our method, called \lloid{}
(Low Latency Online Inspiral Detection),
involves two transformations of the templates that produce a
network of orthogonal filters that is far more computationally
efficient than the original bank of matched filters.

The first transformation is to chop the templates into disjointly supported
intervals, or \emph{time slices}.  Since the time slices of a given template
are disjoint in time, they are orthogonal with respect to time.  Given the
chirp-like structure of the templates, the ``early'' (lowest frequency) time
slices have significantly lower bandwidth and can be safely downsampled.
Downsampling reduces the total number of filter coefficients by a factor of
$\sim$100 by treating the earliest part of the waveform at $\sim$$1/100$ of
the full sample rate.  Together, the factor of 100 reduction in the number of
filter coefficients and the factor of 100 reduction in the sample rate save a
factor of $\sim$$10^4$ floating point operations per second (\flops) over the
original (full sample rate) templates.

However, the resulting filters are still not
orthogonal across the parameter space, and are in fact highly redundant.
We use the \SVD{} to approximate the template bank by a set of orthogonal
\emph{basis filters}~\citep{Cannon:2010p10398}.  We find that this approximation
reduces the number of filters needed by another factor of $\sim$100.  These two
transformations combined reduce the number of floating point operations
to the level that is competitive with the conventional high-latency \FD\
matched filter approach.  In the remainder of this section we describe the
\lloid\ algorithm in detail and provide some basic computational cost scaling.

\subsubsection{Selectively reducing the sample rate of the data and templates}
\label{sec:time-slices}

The first step of our proposed method is to divide the templates into time
slices in a \TD{} analogue to the \FD{} decomposition described
in \citet{Marion2004} and \citet{Buskulic2010}.  We decompose each template
$h_{i}[k]$ into a sum of $S$ non-overlapping templates
%
\begin{equation}
\label{eq:time-slices}
h_{i}[k] = \sum_{s=0}^{S-1}
	\begin{cases}
		h_i^s[k] & \textrm{if } t^s \leqslant k / f^0 < t^{s+1} \\
		0 & \textrm{otherwise}
	\end{cases}
\end{equation}
%
for $S$ integers $\{f^0 t^s\}$ such that $0  = f^0 t^0 < f^0 t^1 < \cdots < f^0
t^S = N$.  The outputs of these new time-sliced filters
form an ensemble of partial \SNR{} streams.  By linearity of the filtering
process, these partial \SNR\ streams can be summed to reproduce the
\SNR\ of the full template.

Since waveforms with neighboring intrinsic source parameters $\mathbf\Theta$
 have similar time-frequency evolution, it is possible to design computationally
efficient time slices for an extended region of parameter space rather than to
design different time slices for each template.

For concreteness and simplicity, consider an inspiral waveform in the
quadrapole approximation, for which the time-frequency relation~\citep{kidder1992, findchirppaper} is
%
\begin{equation} \label{eq:fgw}
%
f(t) = \frac{1}{\pi \mathcal{M}} \left[ \frac{5}{256}\frac{\mathcal{M}}{t}
\right]^{3/8}.
%
\end{equation}
%
Here, $\mathcal{M}$ is the chirp mass of the binary in units of time (where $G
M_\odot / c^3 \approx 5$~$\upmu$s) and $t$ is the time relative to the
coalescence of the binary.
This monotonic time-frequency relationship allows us
to choose time slice boundaries that require substantially less bandwidth at
early times in the inspiral.

An inspiral signal will enter the detection band with some low frequency
$f_\mathrm{low}$ at time $t_\mathrm{low}$ before merger.  Usually the template
is truncated at some prescribed time $t^0$, or equivalent frequency $f_\mathrm{hi}$,
often chosen to correspond to the \ISCO. The beginning of the template is critically
sampled at $2 f_\mathrm{low}$, but the end of the template is critically sampled at a
rate of $2 f_\mathrm{hi}$. In any time interval smaller than the duration of the template,
the bandwidth of the filters across the entire template bank may be significantly less
than the full sample rate at which data is acquired.

Our goal is to reduce the filtering cost of a
large fraction of the waveform by computing part of the convolution at a lower
sample rate.  Specifically we consider here time slice boundaries with the
smallest power-of-two sample rates that sub-critically sample the time-sliced
templates.  The time slices consist of the $S$ intervals
$\left[t^0, t^1\right),\, \left[t^1, t^2\right),\, \dots,\, \left[t^{S-1}, t^S\right)$,
sampled at frequencies $f^0,\, f^1,\, \dots,\, f^\mathrm{S-1}$ where $f^s$ is at
least twice the highest nonzero frequency component of any filter in the bank for the
$s$th time slice.

The time sliced templates may then be downsampled in each interval without
aliasing, so we define them as
%
\begin{equation}
\label{eq:time-sliced-templates}
h_{i}^{s}[k] \equiv
	\begin{cases}
		h_{i}\!\left[k\frac{f}{f^s}\right] & \textrm{if } t^s \leqslant k/f^s < t^{s+1} \\
		0 & \textrm{otherwise.}
	\end{cases}
\end{equation}
%
We note that the time slice decomposition in equation~(\ref{eq:time-slices}) is
manifestly orthogonal since the time slices are disjoint in time.  In the next
section we examine how to reduce the number of filters within each time slice
via \SVD{} of the time-sliced templates.

\subsubsection{Reducing the number of filters with the \SVD{}}
\label{sec:svd}

As noted previously, the template banks used in inspiral searches are by design
highly correlated.  \citet{Cannon:2010p10398} showed that applying the \SVD\
to inspiral template banks greatly reduces the number of filters required to achieve a
particular minimal match.  A similar technique can be applied to the time-sliced
templates as defined in equation~\ref{eq:time-sliced-templates} above.  The \SVD\
is a matrix factorization that takes the form
%
\begin{equation}
h_i^s[k] = \sum_{\mathclap{l=0}}^{\mathclap{M-1}} v_{il}^s \sigma_l^s u_l^s[k] \approx \sum_{\mathclap{l=0}}^{\mathclap{L^s-1}} v_{il}^s \sigma_l^s u_l^s[k].
\label{eq:svddecomp}
\end{equation}
where $u_l^s[k]$ are orthonormal \emph{basis templates} related to the original
time-sliced templates through the \emph{reconstruction matrix}, $v_{il}^s\sigma_l^s$.
The expectation value of the fractional loss in \SNR\ is the \SVD\ tolerance, given by
%
\begin{equation*}
\left[ \sum_{l=0}^{L^s-1} \left( \sigma_l^s \right)^2 \right]\left[ \sum_{l=0}^{M-1} \left( \sigma_l^s \right)^2 \right]^{-1},
\end{equation*}
%
determined by the number $\numsvdtmps$ of basis templates that are kept in
the approximation.  The authors of \citet{Cannon:2010p10398}
showed that highly accurate approximations of inspiral template banks could be
achieved with few basis templates.  We find that when combined with the
time slice decomposition, the number of basis templates \numsvdtmps\ is much
smaller than the original number of templates \numtmps\ and improves on the
rank reduction demonstrated in \citep{Cannon:2010p10398} by nearly an order
of magnitude.  In the next section we describe how we form our early-warning
detection statistic using the time slice decomposition and the \SVD.

\subsubsection{Early-warning output}

In the previous two sections we have described two transformations that greatly
reduce the computational burden of \TD\ filtering.  We are now prepared to define
our detection statistic, the early-warning output, and to comment on the computational
cost of evaluating it.

First, the sample rate of the detector data must be decimated to match sample rates
with each of the time slices.  We will denote the decimated detector data streams using
a superscript $^s$ to indicate the time slices to which they correspond, and the
operator $H^\shortdownarrow$ will represent the appropriate decimation filter:
\begin{equation*}
\label{eq:decomp}
	x^{s+1}[k] = \left( H^\shortdownarrow x^s\right)[k].
\end{equation*}
We shall use the symbol $H^\shortuparrow$ to represent an interpolation filter,
\begin{equation*}
	x^{s}[k] = \left( H^\shortuparrow x^{s+1}\right)[k].
\end{equation*}
Ultimately, the latency of the entire \lloid\ algorithm is set by the decimation
and interpolation filters because they are generally time-symmetric.  As a result,
\lloid\ is very slightly acausal with an intrinsic latency of
$\left(\max \left( N^\shortdownarrow, N^\shortuparrow \right) / 2 f^0 \right)$~seconds.

From the combination of the time slice decomposition in
equation~(\ref{eq:time-sliced-templates}) and the \SVD\ defined in
equation~(\ref{eq:svddecomp}), we define the early-warning output accumulated
up to time slice $s$ using the recursion relation,
%
% orthogonal decomposition filtering
%
\begin{multline}
	\rho_i^s [k] =%
		% Interpolation SNR
		\overbrace{
			\left(H^\uparrow \rho_i^{s+1}\right)[k]
		}^\textrm{\clap{{\sc snr} from previous time slices}} \\
		% Plus ...
		+
		% Reconstruction
		\underbrace{
			\sum_{\mathclap{l=0}}^{\mathclap{L^s-1}} v_{il}^s \sigma_l^s
		}_\textrm{\clap{reconstruction}}
		% Orthogonal FIR filter
		\overbrace{
			\sum_{\mathclap{n=0}}^{\mathclap{N^s-1}} u_l^s[n] x^s[k-n]
		}^\textrm{\clap{orthogonal {\sc fir} filters}} .
\end{multline}
%
%
Observe that the early-warning output for time slice 0, $\rho_i^0[k]$,
approximates the \SNR\ of the original templates.
%
%
\begin{figure*}[h!]
	\begin{center}
		\includegraphics{figures/lloid-diagram}
		\caption{\label{fig:pipeline} Schematic of \lloid{} pipeline illustrating
signal flow.  Circles with arrows represent interpolation
\protect\includegraphics{figures/upsample-symbol} or decimation
\protect\includegraphics{figures/downsample-symbol}.  Circles with plus
signs represent summing junctions
\protect\includegraphics{figures/adder-symbol}.  Squares
\protect\includegraphics{figures/fir-symbol} stand for \fir{} filters.  Sample
rate decreases from the top of the diagram to the bottom.  In this diagram each
time slice contains three \fir\ filters that are linearly combined to produce
four output channels.  In a typical pipeline the number of \fir\ filters is
much less than the number of output channels.}
	\end{center}
\end{figure*}
%
%
The signal flow diagram in Figure~\ref{fig:pipeline} illustrates this
recursion relation as a multirate filter network with a number of early-warning outputs.

In the next section we compute the expected computational cost scaling of this
decomposition and compare it with the brute-force \TD{} implementation of
\eqref{eq:SNRTD} and higher latency \FD{} methods.

\subsection{Comparison of computational costs}

We now examine the computational cost scaling of the conventional \TD{} or
\FD{} matched filter procedure as compared with \lloid{}.  For convenience,
Table~\ref{tab:recap} provides a review of the notation that we will need in
this section.
%
% symbols used in FLOPs calculations table
%
\begin{table}
\caption{\label{tab:recap}Notation used to describe filters.}
\begin{center}
\begin{tabular}{ll}
\tableline\tableline
& Definition \\
\tableline
$f^s$		& sample rate in time slice $s$ \\
\numtmps		& num. templates \\
\tmpsamps	& num. samples per template \\
\numslices	& num. time slices \\
\numsvdtmps	& num. basis templates in time slice $s$ \\
\slicessamps	& num. samples in decimated time slice $s$\\
$N^\shortdownarrow$ & num. coefficients in decimation filter \\
$N^\shortuparrow$ & num. coefficients in interpolation filter \\
\tableline
\end{tabular}
\end{center}
\end{table}


\subsubsection{Conventional \TD{} method}

The conventional \TD{} method consists of a bank of \fir{} filters, or sliding-window dot products.  If there are $\numtmps$ templates, each $\tmpsamps$ samples in length, then each filter requires $M N$ multiplications and additions per sample, or $2 \numtmps \tmpsamps f^0$ \flops\ at a sample rate $f^0$.

\subsubsection{Conventional \FD{} method}

The most common \FD{} method is known as the \emph{overlap-save} algorithm, described in
\citet{numerical-recipes-chapter-13}.  It entails splitting the input into blocks of $D$
samples, $D > \tmpsamps$, each block overlapping the previous one by $D - \tmpsamps$
samples.  For each block, the algorithm computes the forward \fft\ of the data and
each of the templates, multiplies them, and then computes the reverse \fft.

Modern implementations of the \fft, such as the ubiquitous \texttt{fftw}, require about
$2 \fftblock \lg \fftblock$ operations to evaluate a real transform of size
$\fftblock$~\citep{Johnson:2007p9654}.  Including the forward transform of the data and
$M$ reverse transforms for each of the templates, the \fft\ costs $2 (\numtmps + 1)
\fftblock \lg \fftblock$ operations per block.  The multiplication of the transforms adds
a further $2 \numtmps \fftblock$ operations per block.  Since each block produces
$\fftblock - \tmpsamps$ usable samples of output, the overlap-save method requires
$$
f^0 \cdot \frac{2 (\numtmps + 1) \lg \fftblock + 2 \numtmps}{1 - \tmpsamps/\fftblock} \; \mathrm{\flops} \,.
$$

In the limit of many templates, $M \gg 1$, we may neglect the cost of the forward
transform of the data and of the multiplication of the transforms.  The computational
cost will reach an optimum at some large but finite \fft\ block size
$\fftblock \gg \tmpsamps$.  In this limit, the \FD\ method costs
$\approx 2 f^0 \numtmps \lg \fftblock$ \flops.

\subsubsection{\lloid\ method}

For time slice $s$, the \lloid\ method requires $2 \slicessamps \numsvdtmps f^s$ \flops\ 
to evaluate the orthogonal filters, $2 \numtmps \numsvdtmps f^s$ \flops\ to apply the 
linear transformation from the $\numsvdtmps$ basis templates to the $\numtmps$ time-sliced templates, and $\numtmps f^s$ \flops\ to add the resultant partial \SNR\ stream.

The computational cost of the decimation of the detector data is a little bit more subtle.  Decimation is achieved by applying an \fir\ anti-aliasing filter and then downsampling, or deleting samples in order to reduce the sample rate from $f^{s-1}$ to $f^s$.  Naively, an anti-aliasing filter with $N^\shortdownarrow$ coefficients should demand $2 N^\shortdownarrow f^{s-1}$ \flops.  However, it is necessary to evaluate the anti-aliasing filter only for the fraction $f^s / f^{s-1}$ of the samples that will not be deleted.  Consequently, an efficient decimator that requires only $2 N^\shortdownarrow f^{s-1} \cdot \left( f^s / f^{s-1} \right) = 2 N^\shortdownarrow f^s$ \flops\ is possible.

The story is similar for the interpolation filters used to match the sample rates of the partial \SNR{} streams.  Interpolation of a data stream from a sample rate $f^s$ to $f^{s-1}$ consists of inserting zeros between the samples of the original stream, and then applying a low-pass filter with $N^\shortdownarrow$ coefficients.  The low-pass filter requires $2 M N^\shortdownarrow f^{s-1}$ \flops.  However, by taking advantage of the fact that by construction a fraction $f^{s-1}/f$ of the samples are zero, it is possible to build an efficient interpolator that requires only $M N^\shortuparrow f^{s-1} \cdot \left( f^s / f^{s-1} \right) = 2 M N^\shortuparrow f^s$ \flops.

Taking into account the decimation of the detector data, the orthogonal \fir\ filters, the reconstruction of the time-sliced templates, the interpolation of \SNR\ from previous time slices, and the accumulation of \SNR, in total the \lloid\ algorithm requires
\begin{multline*}
\sum_{\mathclap{s=0}}^{\mathclap{S-1}} \left( 2 \slicessamps \numsvdtmps + 2 \numtmps \numsvdtmps + \numtmps \right) f^s \\ + 2\sum_{\mathclap{f \in \{f^s\}}} \left( N^\shortdownarrow + \numtmps N^\shortuparrow \right) f^s
\end{multline*}
\flops.  The second sum is carried out over the set of distinct sample rates rather than over the time slices themselves.

We can simplify this expression quite a bit by taking some limits that arise from sensible filter design.  In the limit of many templates, the cost of the decimation filters is negligible as compared to the cost of the interpolation filters.  Typically, we will design the interpolation filters with $N^\shortuparrow \lesssim \numsvdtmps$ so that the interpolation cost itself is negligible compared with the reconstruction cost.  Finally, if the number of basis templates per time slices $\numsvdtmps$ is not too small, the reconstruction cost dominates over the cost of accumulating the partial \SNR.  In these limits, the cost of \lloid\ is dominated by the basis filters themselves and the reconstruction, totaling $2 \sum_{s=0}^{S-1} f^s \numsvdtmps \left( \slicessamps + \numtmps \right)$ \flops.

\subsubsection{Speedup of \lloid\ relative to \TD\ method}

If the cost of the basis filters dominates, and the frequency of the templates evolves slowly enough in time, then we can use the time-frequency relationship of equation~(\ref{eq:fgw}) to estimate the speedup relative to the conventional \TD\ method.  The reduction in \flops\ is approximately
%
\begin{multline}
\label{eq:speedup}
\frac{2 \sum_{s=0}^{S-1} f^s \numsvdtmps \slicessamps}{2 \numtmps \tmpsamps f^0} \\
\approx \frac{\alpha}{\left(t_\mathrm{low} - t_\mathrm{hi}\right) \left(f^0\right)^2} \int_{t_\mathrm{low}}^{t_\mathrm{hi}} \left(2 f(t) \right)^2 \, \mathrm{d} t \\
= \frac{16 \alpha \left(t_\mathrm{low} f^2 (t_\mathrm{low}) - t_\mathrm{hi} f^2 (t_\mathrm{hi}) \right)}{\left(f^0\right)^2 \left(t_\mathrm{low} - t_\mathrm{hi}\right)}
\end{multline}
%
where $\alpha \approx \numsvdtmps / \numtmps$ is the rank reduction factor, or ratio between the number of basis templates and the number of templates.  This approximation assumes that the frequency of the signal is evolving very slowly so that we can approximate the time slice sample rate as twice the instantaneous gravitational wave frequency, $f^s \approx 2 f(t)$, and the number of samples in the decimated time slice as the sample rate times an infinitesimally short time interval, $\slicessamps \approx 2 f(t) \, \mathrm{d}t$. The integral is evaluated using the power-law form of $f(t)$ from equation~(\ref{eq:fgw}). Substituting approximate values for a template bank designed for component masses around (1.4, 1.4) $M_\odot$, $\alpha \approx 10^{-2}$, $t_\mathrm{low} = 10^3$~s, $f_\mathrm{low} = 10^1$~Hz, $t_\mathrm{hi} = 1/1570$~s $f_\mathrm{hi} = 1570$~Hz, and $f^0 = (2)(1570)$~Hz, we find from equation~(\ref{eq:speedup}) that the \lloid\ method requires only $\sim 10^{-6}$ times as many \flops\ as the conventional \TD\ method.
